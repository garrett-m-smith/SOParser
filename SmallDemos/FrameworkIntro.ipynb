{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Organizing Sentence Processing in a Harmony Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a featural lexical-dependency grammar, we design a Harmony Function (HF) that specifies the harmony (well-formedness) of possible combination of features and dependency links between words. The function is defined on a space of link and treelet feature variables. The gradient of HF gives the dynamics of processing, i.e., it specifies the way the link and feature values change from any initial state. Specifically, the processing dynamics can be thought of as always moving \"uphill\" on the harmony surface until the state reaches a local maximum of well-formedness. This approach to sentence processing is closely related to the approach outlined in a series of papers involving Pyeong Whan Cho, Paul Smolensky, Matthew Goldrick, and Richard Lewis (Cho et al., 2016, 2017, 2018). The Cho et al. approach also models sentence processing as navigation on a harmony landscape, but their harmony function and processing dynamics are somewhat more complicated. Here, we illustrate the main thrust of the harmony landscape approach and provide a simple example of sentence processing in our SOSP approach (<u>S</u>elf-<u>O</u>rganizing <u>S</u>entence <u>P</u>rocessing; Smith, Franck, & Tabor, accepted; Villata, Tabor, & Franck, submitted).\n",
    "\n",
    "Sentence comprehension in SOSP consists of (c1) starting with the system in an initial state in which all variables (feature values and attachment link strengths) have the value 0, (c2) perceiving a word, which results in the setting of a subset of treelet features to non-zero values, (c3) gravitating under the dynamics + noise to a low velocity threshold (this generally results in nearly reaching a local harmony peak), and repeating from step (c2) until all the words in the sequence to be comprehended have been processed. Random production (useful for probing the non-contextualized distribution over parse trajectories) is accomplished by (p1) starting in the zero state, (p2) using the prior of initial word likelihoods to select a word, (p3) activating the treelet features associated with that word, (p4) gravitating under the dynamics + noise to the low velocity threshold, (p5) generating a distribution over next words from the ensemble of open lexical attachment site activations, (p6) generating a new word, and repeating from step (p3) until (p4) has produced maximal harmony or a practical limit on sentence length has been exceeded.  (The case of production of language in a discourse context will be discussed separately.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar is a collection of lexically anchored treelets.  Each treelet consists of a mother and a finite number of daughters, some of which may be marked \"optional\".  The mother and daughters are vectors of features, all of the same dimension, with the same interpretation sasigned to each dimension. For example, here is a feature vector that supports the encoding of several basic lexical types:\n",
    "\n",
    "    Noun\n",
    "    Verb\n",
    "    Preposition\n",
    "    Determiner\n",
    "    Singular\n",
    "    Plural\n",
    "    Phonological or orthographic indices (these are indexical bit vectors for all the words in the language)\n",
    "    \n",
    "The values of these features are taken to range from -1 to +1, with -1 meaning absence, and +1 presence of the property indicated by the feature. For example, the treelets for the words \"the\", \"dog\", \"cats\", \"near\", and \"sleeps\" as well as a special \"Root\" treelet, which has no mother and licenses the matrix sentence daughter, are shown in Table 1.\n",
    "\n",
    "Table 1.  A lexicon of treelets for a fragment of English.\n",
    "\n",
    "    Label:          dog\n",
    "    Mother:         1 0 0 0 1 0 0P # The phrase headed by \"dog\" is nominal and singular\n",
    "    Daughter 1:     0 0 0 1 1 0 0P # Singular determiner comes first\n",
    "    Daughter 2:     0 0 0 0 0 0 \"dog\" # The phonological/orthographic pointer to \"dog\"\n",
    "    (Daughter 3):   0 0 1 0 0 0 0P # The noun can optionally be followed by a prepositional phrase\n",
    "    \n",
    "    Label:          cats\n",
    "    Mother:         1 0 0 0 0 1 0P # The phrase headed by \"cats\" is nominal and plural\n",
    "    (Daughter 1:)   0 0 0 1 0 1 0P # Plural determiner optionally comes first\n",
    "    Daughter 2:     0 0 0 0 0 0 \"cats\" # The phonological/orthographic pointer to \"cats\"\n",
    "    (Daughter 3):   0 0 1 0 0 0 0P # The noun can optionally be followed by a prepositional phrase\n",
    "    \n",
    "    Label:          near\n",
    "    Mother:         0 0 1 0 0 0 0P # The phrase headed by \"near\" is a prepositional phrase\n",
    "    Daughter 1:     0 0 0 0 0 0 \"near\" # The phonological/orthographic pointer to \"near\"\n",
    "    Daughter 2:     1 0 0 0 0 0 0P # The preposition must have a Noun Phrase object\n",
    "\n",
    "    Label:          sleeps\n",
    "    Mother:         0 1 0 0 1 0 0P # The phrase headed by \"sleeps\" is verbal and singular\n",
    "    Daughter 1:     1 0 0 0 1 0 0P # The subject comes first and must be singluar\n",
    "    Daughter 2:     0 0 0 0 0 0 \"sleeps\" # The phonological/orthographic pointer to \"sleeps\"\n",
    "    \n",
    "    Label:          the\n",
    "    Mother:         0 0 0 1 - - 0P # The phrase headed by \"the\" is a determiner phrase of flexible number\n",
    "    Daughter 1:     0 0 0 0 0 0 \"the\" # The phonological/orthographic pointer to \"the\"\n",
    "    \n",
    "    Label:          Root\n",
    "    Daughter 1:     0 1 0 0 0 0 0P # The daughter of root must be a verb-headed structure\n",
    "     \n",
    "The treelets can combine in any way that allows their features to unify, subject to the constraints that daughters can only combine with mothers, mothers can only combine with daughters, and all mothers and non-optional daughters must combine with one and only one vector. Wild card features (-) can match any value.  When vectors including wild-card features combine, the wild-card features take on the values of their partners in unification.  This system supports recursive combination:  the combination tops at out at Root and bottoms out at the phonological/orthographic forms, which have no daughters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the grammar as a set of lexical entries (leaving off phonology/orthography for the time being):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class LexEntry:\n",
    "    def __init__(self, name, mother, dlist, head, ddir, dopt, dlabel):  \n",
    "                    # name, mother vector, daughter matrix (1 per row), head daughter, optionality of daughters\n",
    "        self.name = name\n",
    "        self.mother = np.array(mother)\n",
    "        self.dlist = np.array(dlist)\n",
    "        self.head = head\n",
    "        self.ddir = np.array(ddir)\n",
    "        self.dopt = np.array(dopt)\n",
    "        self.dlabel = dlabel\n",
    "        \n",
    "## Build the lexicon (daughter order information (ddir) is left empty in the manual entries, computed below)\n",
    "word_dog =     LexEntry('dog',    [1, 0, 0, 0, 1, 0],   [(0, 0, 0, 1, 1, 0), \n",
    "                                                         (0, 0, 0, 0, 0, 0), \n",
    "                                                         (0, 0, 1, 0, 0, 0)], 1, [], [0, 0, 1], ['DetP',\n",
    "                                                                                                 'PO',\n",
    "                                                                                                 'PP'])\n",
    "word_cats =     LexEntry('cats',   [1, 0, 0, 0, 0, 1],   [(0, 0, 0, 1, 0, 1), \n",
    "                                                         (0, 0, 0, 0, 0, 0), \n",
    "                                                         (0, 0, 1, 0, 0, 0)], 1, [], [1, 0, 1], ['DetP',\n",
    "                                                                                                 'PO',\n",
    "                                                                                                 'PP'])\n",
    "word_near =    LexEntry('near',   [0, 0, 1, 0, 0, 0],   [(0, 0, 0, 0, 0, 0),\n",
    "                                                         (1, 0, 0, 0, 0, 0)], 0, [], [0, 0],    ['PO',\n",
    "                                                                                                 'NP(Obj)'])\n",
    "word_sleeps =  LexEntry('sleeps', [0, 1, 0, 0, 1, 0],   [(1, 0, 0, 0, 1, 0),\n",
    "                                                         (0, 0, 0, 0, 0, 0)], 1, [], [0, 0],    ['NP(Subj)',\n",
    "                                                                                                 'PO'])\n",
    "word_the =     LexEntry('the',    [0, 0, 0, 1, .5, .5], [(0, 0, 0, 0, 0, 0)], 0, [], [0],       ['PO'])\n",
    "word_Root =    LexEntry('Root',   [],                   [(0, 1, 0, 0, 0, 0)], 2, [], [0],       ['VP']) # Head pos = 2\n",
    "                                                                                                # implies VP daughter\n",
    "                                                                                                # is leftward attaching\n",
    "\n",
    "lexicon01 = [word_dog, word_cats, word_near, word_sleeps, word_the, word_Root]\n",
    "featurecount01 = np.shape(lexicon01[0].dlist)[1]\n",
    "\n",
    "## Add the order info based on the position of the head\n",
    "for ind, word in enumerate(lexicon01):\n",
    "    word.ddir = np.sign(np.array(range(len(word.dopt))) - word.head)\n",
    "    \n",
    "## Probe an entry\n",
    "#lexicon01[3].dlabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing first on comprehension, we consider all possible word sequences drawn from the vocabulary:\n",
    "\n",
    "    the\n",
    "    dog\n",
    "    cats\n",
    "    sleeps\n",
    "    the dog\n",
    "    the cats\n",
    "    the sleeps\n",
    "    etc.\n",
    "    \n",
    "Each word spawns a treelet which affords particular, ordered attachments. So, for a given sequence, we consider all attachments consistent with the order constraints of the treelets.  For example, in \n",
    "\n",
    "    the cats dog\n",
    "\n",
    "the following links are entertained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1.  Possible links (dashed lines) between nodes for \"the cats dog\". The arrows on the daughter nodes indicate the direction that daughter treelet should be found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"Figures/the-cats-dog.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10bb9e4a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "print('Figure 1.  Possible links (dashed lines) between nodes for \"the cats dog\". The arrows on the daughter nodes indicate the direction that daughter treelet should be found.')\n",
    "IFrame(\"Figures/the-cats-dog.pdf\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider link-value assignments.  Link-values generally range from around 0 to around 1, but for the purpose of defining HF, we consider only values equal to 0 or 1, and we consider only value-assignments that obey the *at-most-one-hot* constraint:  at most one fully activated link can emanate from each attachment site. (One might use a stochastic dynamical system to find most of the link combinations that obey these constraints; for the example construction below, we generated all 0/1 assignments and then eliminated those that failed to obey *at-most-one-hot*.)\n",
    "\n",
    "To find the allowed link assignments, we first generate all links, then select allowed link assignments from among these. A link is defined as a connection between an attachment site on a treelet-instance to an attachment site on a different treelet-instance.\n",
    "\n",
    "The indexing of the links takes the following form:\n",
    "\n",
    "[[sentence-position$_1$ word-type$_1$ site-index$_1$] [sentence-position$_2$ word-type$_2$ site-index$_2$]]\n",
    "\n",
    "where sentence-position counts by words, starting at 0, from the beginning of the sentence, word-type is the index of the word's class in the lexicon, and site-index gives the attachment site within the treelet (-1 = mother, 0 = first daughter, 1 = second daughter, etc.).  For example, the link from the mother of \"the\" to the DetP of \"dog\" in the diagram above is coded:\n",
    "\n",
    "[[0 4 -1] [2 0 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 4, -1], [1, 1, 0]],\n",
       " [[0, 4, -1], [2, 0, 0]],\n",
       " [[1, 1, -1], [2, 0, 0]],\n",
       " [[1, 1, 2], [2, 0, -1]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For the string in question, find the possible links\n",
    "\n",
    "import re\n",
    "def get_words(text):\n",
    "    return re.compile('\\w+').findall(text)\n",
    "\n",
    "def find_in_list(item, tlist):\n",
    "    try:\n",
    "        return tlist.index(item)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "## Get stringlist indices of words in currstring\n",
    "def get_str_inds(currstring, stringlist):\n",
    "    strinds = []\n",
    "    for ind, ww in enumerate(currstring):\n",
    "        currind = find_in_list(ww, stringlist)\n",
    "        strinds.append(currind)\n",
    "    return strinds\n",
    "\n",
    "## Enumerate possible links (ignoring links to Phonology/Orthography)\n",
    "def get_links(instring, currlex):\n",
    "    currstring = get_words(instring)\n",
    "    namelist = list(x.name for x in currlex)\n",
    "    tlinds = get_str_inds(currstring, namelist)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    linklist = []\n",
    "    for ind1, w1 in enumerate(currstring):\n",
    "        substring = list(currstring[(ind1+1):])\n",
    "        ## Mother of current treelet connects to left-pointing daughters of subsequent treelets\n",
    "        if len(currlex[tlinds[ind1]].mother) > 0:  # The Root node has no mother so no links should be made there\n",
    "            mtlind = tlinds[ind1]\n",
    "            basenode = [ind1, tlinds[ind1], -1] # Mother of current\n",
    "            for ind2, w2 in enumerate(substring):\n",
    "                stringpoint = ind1 + ind2 + 1\n",
    "                currtlind = tlinds[stringpoint]\n",
    "                for dind, currdir in enumerate(currlex[currtlind].ddir):\n",
    "                    if currdir == -1:  # Daughter must be a left daughter\n",
    "                        linklist.append([basenode, [stringpoint, currtlind, dind]])\n",
    "        ## Daughters of current treelets point to mothers of subsequent treelets\n",
    "        for dind, currdir in enumerate(currlex[mtlind].ddir): \n",
    "            basenode = [ind1, mtlind, dind]\n",
    "            if currdir == 1:\n",
    "                for ind2, w2 in enumerate(substring):\n",
    "                    stringpoint = ind1 + ind2 + 1\n",
    "                    currtlind = tlinds[stringpoint]\n",
    "                    if len(currlex[currtlind].mother) > 0:   # The Root node has no mother so no links should be made there\n",
    "                        linklist.append([basenode, [stringpoint, currtlind, -1]])\n",
    "        \n",
    "    return linklist\n",
    "\n",
    "\n",
    "\n",
    "currstring = 'the cats dog'\n",
    "wordlist = get_words(currstring)\n",
    "namelist = list(x.name for x in lexicon01)\n",
    "tlinds01 = get_str_inds(wordlist, namelist)\n",
    "\n",
    "## Find all possible links in the current lexicon\n",
    "linklist01 = get_links(currstring, lexicon01)\n",
    "linklist01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove the links that fail to obey at-least-one-hot. To support convenient computation of Harmony values later, we construct a special link to a null node for those nodes in Figure 1 which otherwise fail to have a link under at-least-one-hot. In other words, treelets that fail to attach to other treelets can be thought of as attaching to the null treelet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2.  Allowed link assignments for \"the cats dog\".  The last 7 columns show links to\n",
      "          the Null Object, which are not depicted in Figure 1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.the(mother)-1.cats(DetP)</th>\n",
       "      <th>0.the(mother)-2.dog(DetP)</th>\n",
       "      <th>1.cats(mother)-2.dog(DetP)</th>\n",
       "      <th>1.cats(PP)-2.dog(mother)</th>\n",
       "      <th>0.the(mother)-Null(Null)</th>\n",
       "      <th>1.cats(mother)-Null(Null)</th>\n",
       "      <th>1.cats(DetP)-Null(Null)</th>\n",
       "      <th>1.cats(PP)-Null(Null)</th>\n",
       "      <th>2.dog(mother)-Null(Null)</th>\n",
       "      <th>2.dog(DetP)-Null(Null)</th>\n",
       "      <th>2.dog(PP)-Null(Null)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.the(mother)-1.cats(DetP)  0.the(mother)-2.dog(DetP)  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         0.0                        0.0   \n",
       "4                         0.0                        1.0   \n",
       "5                         0.0                        1.0   \n",
       "6                         1.0                        0.0   \n",
       "7                         1.0                        0.0   \n",
       "8                         1.0                        0.0   \n",
       "9                         1.0                        0.0   \n",
       "\n",
       "   1.cats(mother)-2.dog(DetP)  1.cats(PP)-2.dog(mother)  \\\n",
       "0                         0.0                       0.0   \n",
       "1                         0.0                       1.0   \n",
       "2                         1.0                       0.0   \n",
       "3                         1.0                       1.0   \n",
       "4                         0.0                       0.0   \n",
       "5                         0.0                       1.0   \n",
       "6                         0.0                       0.0   \n",
       "7                         0.0                       1.0   \n",
       "8                         1.0                       0.0   \n",
       "9                         1.0                       1.0   \n",
       "\n",
       "   0.the(mother)-Null(Null)  1.cats(mother)-Null(Null)  \\\n",
       "0                       1.0                        1.0   \n",
       "1                       1.0                        1.0   \n",
       "2                       1.0                        0.0   \n",
       "3                       1.0                        0.0   \n",
       "4                       0.0                        1.0   \n",
       "5                       0.0                        1.0   \n",
       "6                       0.0                        1.0   \n",
       "7                       0.0                        1.0   \n",
       "8                       0.0                        0.0   \n",
       "9                       0.0                        0.0   \n",
       "\n",
       "   1.cats(DetP)-Null(Null)  1.cats(PP)-Null(Null)  2.dog(mother)-Null(Null)  \\\n",
       "0                      1.0                    1.0                       1.0   \n",
       "1                      1.0                    0.0                       0.0   \n",
       "2                      1.0                    1.0                       1.0   \n",
       "3                      1.0                    0.0                       0.0   \n",
       "4                      1.0                    1.0                       1.0   \n",
       "5                      1.0                    0.0                       0.0   \n",
       "6                      0.0                    1.0                       1.0   \n",
       "7                      0.0                    0.0                       0.0   \n",
       "8                      0.0                    1.0                       1.0   \n",
       "9                      0.0                    0.0                       0.0   \n",
       "\n",
       "   2.dog(DetP)-Null(Null)  2.dog(PP)-Null(Null)  \n",
       "0                     1.0                   1.0  \n",
       "1                     1.0                   1.0  \n",
       "2                     0.0                   1.0  \n",
       "3                     0.0                   1.0  \n",
       "4                     0.0                   1.0  \n",
       "5                     0.0                   1.0  \n",
       "6                     1.0                   1.0  \n",
       "7                     1.0                   1.0  \n",
       "8                     0.0                   1.0  \n",
       "9                     0.0                   1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Generate all bit vectors in R^{Ndim}\n",
    "def hypercube(ndim):\n",
    "    if (ndim == 1):\n",
    "        hc = np.array([[0, 1]]).T\n",
    "    else:\n",
    "        subcube = hypercube(ndim-1)\n",
    "        halfsize = 2**(ndim-1)\n",
    "        hczeros = np.concatenate((np.array([np.zeros(halfsize)]).T, subcube), axis = 1)\n",
    "        hcones = np.concatenate((np.array([np.ones(halfsize)]).T, subcube), axis = 1)\n",
    "        hc = np.concatenate((hczeros, hcones), axis = 0)\n",
    "    return hc\n",
    "\n",
    "## Unique elements of any list\n",
    "def unique_elements(currlist):\n",
    "    uu = []\n",
    "    for item in currlist:\n",
    "        if item not in uu:\n",
    "            uu.append(item)\n",
    "    return uu\n",
    "\n",
    "## Unique nodes in a linklist (a list of pairs of nodes)\n",
    "def get_unique_nodes(linklist):   # Question: is it most efficient to do this repeatedly?\n",
    "    nodelist = []\n",
    "    for currlink in linklist:\n",
    "        nodelist.append(currlink[0])\n",
    "        nodelist.append(currlink[1])\n",
    "    return unique_elements(nodelist)\n",
    "\n",
    "## Remove link configurations that violate at-most-one-hot\n",
    "def remove_trop_hot(linkacts, linklist):\n",
    "    newacts = []\n",
    "    for config in linkacts:\n",
    "        sublinks = [linklist[ind] for ind, val in enumerate(config) if val]  # Convolve config with linklist\n",
    "        subnodes = get_unique_nodes(sublinks)\n",
    "        allgood = 1\n",
    "        if len(subnodes) > 0:\n",
    "            nodeind = 0\n",
    "            while (nodeind < len(subnodes)) & allgood:\n",
    "                currnode = subnodes[nodeind]\n",
    "                ncount = 0\n",
    "                linkind = 0\n",
    "                while (linkind < len(sublinks)) & (ncount < 2):\n",
    "                    currlink = sublinks[linkind]\n",
    "                    if currlink[0] == currnode:\n",
    "                        ncount += 1\n",
    "                    if currlink[1] == currnode:\n",
    "                        ncount +=1\n",
    "                    linkind += 1\n",
    "                if ncount >= 2:\n",
    "                    allgood = 0\n",
    "                nodeind += 1\n",
    "        if allgood:\n",
    "            newacts.append(config)\n",
    "#     return newacts\n",
    "    return np.array(newacts)\n",
    "\n",
    "def convolve_lists(ilist, tlist):\n",
    "    ### Select the subset of tlist for which the corresponding ilist indices are not 0\n",
    "    return [tlist[ind] for ind, val in enumerate(ilist) if val]\n",
    "    \n",
    "def find_in_array(tval, sarray):\n",
    "    return np.where((sarray == tval).all(axis=1))\n",
    "    \n",
    "def get_nodes(tlinds, lexicon):\n",
    "    nodelist = []\n",
    "    for tlindind, tlind in enumerate(tlinds):\n",
    "        nodelist.append([tlindind, tlind, -1]) # The mother\n",
    "        for dind, dopt  in enumerate(lexicon[tlind].dopt):\n",
    "            if dind != lexicon[tlind].head:\n",
    "                nodelist.append([tlindind,tlind, dind]) # The daughters (optional are included, to be ignored later as needed)\n",
    "    return nodelist\n",
    "\n",
    "nullsite = [-2, -2, -2]\n",
    "\n",
    "## Add null attachment for nodes that are non-optional but lack an activated link\n",
    "def add_unattached(linkacts, linklist, lexicon, tlinds):\n",
    "    # Check each link configuration\n",
    "    currnodelist = get_nodes(tlinds, lexicon)\n",
    "    newacts = np.zeros((np.shape(linkacts)[0], np.shape(linkacts)[1]+len(currnodelist)))\n",
    "    for cind, config in enumerate(linkacts):\n",
    "        supconfig = np.zeros(len(currnodelist))\n",
    "        currlinks = convolve_lists(config, linklist)\n",
    "        curractivenodes = get_unique_nodes(currlinks)\n",
    "        for tlindind, currtl in enumerate(tlinds):\n",
    "            if len(lexicon[currtl].mother) > 0:  # If node does not have a mother, then do not link mother to nullsite\n",
    "                mothernode = [tlindind, currtl, -1]\n",
    "                if find_in_list(mothernode, curractivenodes) == -1:\n",
    "                    nullindex = find_in_list(mothernode, currnodelist)\n",
    "                    supconfig[nullindex] = 1\n",
    "            for dind, dopt in enumerate(lexicon[currtl].dopt):\n",
    "                if dind != lexicon[currtl].head:\n",
    "                    daughternode = [tlindind, currtl, dind]\n",
    "                    if find_in_list(daughternode, curractivenodes) == -1:\n",
    "                        nullindex = find_in_list(daughternode, currnodelist)\n",
    "                        supconfig[nullindex] = 1\n",
    "                        \n",
    "        #import pdb; pdb.set_trace()    \n",
    "                        \n",
    "        newconfig = np.concatenate([config, supconfig])\n",
    "        newacts[cind, :] = newconfig   \n",
    "        \n",
    "    newlinks = []\n",
    "    for node in currnodelist:\n",
    "        newlinks.append([node, nullsite])\n",
    "    linklist = linklist + newlinks\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "\n",
    "    return (newacts, linklist) \n",
    "\n",
    "## Generate all absolute link value assignments, then remove violations\n",
    "def allowed_configs(linklist, lexicon, tlinds):\n",
    "    # Check each configuration to see if it satisfies at-most-one-hot\n",
    "    ## Assign absolute activation patterns to the linklist\n",
    "    linkacts = hypercube(np.shape(linklist)[0])\n",
    "    ## Reduce to just those patterns which satisfy at-most-one-hot\n",
    "    linkacts = remove_trop_hot(linkacts, linklist) \n",
    "    ## Add links for unattached \n",
    "    return add_unattached(linkacts, linklist, lexicon, tlinds)  # Note:  returns updated linkacts AND linklist\n",
    "\n",
    "## Label the links\n",
    "def make_labels(linklist, lexicon, tlinds):\n",
    "    linklabels = []\n",
    "    for currlink in linklist:\n",
    "        \n",
    "#         if currlink[0][0] > 2:\n",
    "#             import pdb; pdb.set_trace()\n",
    "        \n",
    "        wordind0 = str(currlink[0][0]) + '.'\n",
    "        phrase0 = lexicon[currlink[0][1]].name\n",
    "        if currlink[0][2] == -1:\n",
    "            site0 = 'mother'\n",
    "        else:\n",
    "            site0 = lexicon[currlink[0][1]].dlabel[currlink[0][2]]\n",
    "        \n",
    "        if currlink[1][0] == -2:\n",
    "            phrase1 = 'Null'\n",
    "            site1 = 'Null'\n",
    "            wordind1 = ''\n",
    "        else:\n",
    "            wordind1 = str(currlink[1][0]) + '.'\n",
    "            phrase1 = lexicon[currlink[1][1]].name\n",
    "            if currlink[1][2] == -1:\n",
    "                site1 = 'mother'\n",
    "            else:\n",
    "                site1 = lexicon[currlink[1][1]].dlabel[currlink[1][2]]\n",
    "        linklabels.append(wordind0 + phrase0 + '(' + site0 + ')' + '-' + wordind1 + phrase1 + '(' + site1 + ')')\n",
    "    return linklabels \n",
    "\n",
    "## Run, format, and print\n",
    "actslinks = allowed_configs(linklist01, lexicon01, tlinds01)\n",
    "allowed01 = actslinks[0]\n",
    "auglinklist01 = actslinks[1]\n",
    "auglinklabels01 = make_labels(auglinklist01, lexicon01, tlinds01)\n",
    "nodelist01 = get_unique_nodes(auglinklist01)  # This is not needed here, but it's useful to refer to\n",
    "\n",
    "#df = pd.DataFrame(allowed01[:, 0:4], index=None, columns = auglinklabels01[0:4])\n",
    "df = pd.DataFrame(allowed01, index=None, columns = auglinklabels01)\n",
    "\n",
    "print('Table 2.  Allowed link assignments for \"the cats dog\".  The last 7 columns show links to')\n",
    "print('          the Null Object, which are not depicted in Figure 1.')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 4, -1],\n",
       " [1, 1, 0],\n",
       " [2, 0, 0],\n",
       " [1, 1, -1],\n",
       " [1, 1, 2],\n",
       " [2, 0, -1],\n",
       " [-2, -2, -2],\n",
       " [2, 0, 2]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodelist01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string, \"the cats dog\", is not, under the current grammar, a grammatical sentence or even a grammatical initial substring (although it is a grammatical initial substring in English as a whole because \"dog\" has a rare interpretation as a transitive verb).   In the current scheme, the \"allowed link assignments\" means those for which the order constraints of the words are obeyed and the at-most-one-hot constraint is obeyed, including but not limited to those that make the string grammatical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in Table 2 gives rise to an upward deflection of $HF$. To accomplish this, we place a radial basis function (RBF) with its center at the state corresponding to the vector of link values. The next step is to determine the magnitude of the upward deflection. For this, we define the local harmony $h_i$ of an absolute link configuration $i$ as:\n",
    "\n",
    "$$h_i = \\prod_{k \\in Links} \\left(act(k) \\frac{|\\sum_{j \\in Features} \\rho(d_{kj}) \\cdot \\rho(m_{kj})|}{|Features|} \\right)$$\n",
    "\n",
    "$$\\rho(x) = 2x - 1$$\n",
    "\n",
    "where $act(k)$ is the activation of Link $k$ (either 0 or 1), $d_j$ is the $j$'th feature value of the daughter node of the link, $m_j$ is the $j$'th feature value of the mother node of the link, $|Features|$ is the number of features in the feature vector, $\\lambda$ is a parameter that specifies the penalty associated with an unattached open site. The feature values used in the evaluation of $h_i$ are those specified in the lexicon of treelets (Table 1), assuming that feature convergence occurs on all links involving wildcards. The local harmony is high if all activated links are associated with good feature matches. \n",
    "\n",
    "The following code produces Table 3, which gives the local harmony values associated with each of the link activation patterns in Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3.  Harmonies associated with the link configurations in Table 2.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.the(mother)-1.cats(DetP)</th>\n",
       "      <th>0.the(mother)-2.dog(DetP)</th>\n",
       "      <th>1.cats(mother)-2.dog(DetP)</th>\n",
       "      <th>1.cats(PP)-2.dog(mother)</th>\n",
       "      <th>Local-Harmonies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.the(mother)-1.cats(DetP)  0.the(mother)-2.dog(DetP)  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         0.0                        0.0   \n",
       "4                         0.0                        1.0   \n",
       "5                         0.0                        1.0   \n",
       "6                         1.0                        0.0   \n",
       "7                         1.0                        0.0   \n",
       "8                         1.0                        0.0   \n",
       "9                         1.0                        0.0   \n",
       "\n",
       "   1.cats(mother)-2.dog(DetP)  1.cats(PP)-2.dog(mother)  Local-Harmonies  \n",
       "0                         0.0                       0.0         0.000000  \n",
       "1                         0.0                       1.0         0.000000  \n",
       "2                         1.0                       0.0         0.000033  \n",
       "3                         1.0                       1.0         0.000000  \n",
       "4                         0.0                       0.0         0.000100  \n",
       "5                         0.0                       1.0         0.000000  \n",
       "6                         0.0                       0.0         0.000001  \n",
       "7                         0.0                       1.0         0.000000  \n",
       "8                         1.0                       0.0         0.003333  \n",
       "9                         1.0                       1.0         0.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rho(vec):\n",
    "    return 2*vec - 1\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "nullmult = 0.01\n",
    "\n",
    "def get_local_harmonies(allowed, auglinklist, currlex):\n",
    "    \n",
    "    ## Debugging\n",
    "    dbline = -1\n",
    "    \n",
    "    #print('Starting local harmony comp.')\n",
    "\n",
    "    hlist = []\n",
    "    for cind, config in enumerate(allowed):\n",
    "        hl = 1  # Local harmony\n",
    "        \n",
    "#         if cind == dbline:\n",
    "#             print('Made it inside cind loop')\n",
    "#             import pdb; pdb.set_trace()\n",
    "            \n",
    "        for lind, currswitch in enumerate(config):\n",
    "            \n",
    "#             if cind == dbline:\n",
    "#                 print('Made it inside lind loop')\n",
    "#                 import pdb; pdb.set_trace()\n",
    "        \n",
    "            if currswitch == 1:\n",
    "                nulllink = 0\n",
    "                currlink = auglinklist[lind]\n",
    "                \n",
    "#                 if cind == dbline:\n",
    "#                     print('Looking at currlink')\n",
    "#                     import pdb; pdb.set_trace()\n",
    "            \n",
    "                if currlink[1][0] == -2:  # The link is to a null node\n",
    "                    if currlink[0][2] == -1:  # It's a mother that's connecting to the null node\n",
    "                        currmult = nullmult\n",
    "                    else:  # It is a daughter that's connecting to the null node\n",
    "                        if currlex[currlink[0][1]].dopt[currlink[0][2]]:  # The daughter is optional\n",
    "                            currmult = 1\n",
    "                        else:\n",
    "                            currmult = nullmult\n",
    "                else:\n",
    "                    \n",
    "#                     if cind == dbline:\n",
    "#                         print('Made it to considering non-null links')\n",
    "#                         import pdb; pdb.set_trace()\n",
    "            \n",
    "                    if currlink[0][2] == -1:  # The left end of the link is a mother\n",
    "                        leftfeat = deepcopy(currlex[currlink[0][1]].mother)\n",
    "                \n",
    "                        if cind == dbline:\n",
    "                            print('Leftfeat: ' + currlex[currlink[0][1]].name + '-' \n",
    "                                + 'mother')\n",
    "\n",
    "                    else:   # The left end of the link is a daughter\n",
    "                        leftfeat = deepcopy(currlex[currlink[0][1]].dlist[currlink[0][2]])\n",
    "                \n",
    "                        if cind == dbline:\n",
    "                            print('Leftfeat: ' + currlex[currlink[0][1]].name + '-' \n",
    "                                + currlex[currlink[0][1]].dlabel[currlink[0][2]])\n",
    "                \n",
    "                    if currlink[1][2] == -1:  # The right end of the link is a mother\n",
    "                        rightfeat = deepcopy(currlex[currlink[1][1]].mother)\n",
    "                \n",
    "                        if cind == dbline:\n",
    "                            print('Rightfeat: ' + currlex[currlink[1][1]].name + '-' \n",
    "                                + 'mother')\n",
    "                    \n",
    "                    else:   # The right end of the link is a daughter\n",
    "                        rightfeat = deepcopy(currlex[currlink[1][1]].dlist[currlink[1][2]])\n",
    "                \n",
    "                        if cind == dbline:\n",
    "                            print('Rightfeat: ' + currlex[currlink[1][1]].name + '-' \n",
    "                                + currlex[currlink[1][1]].dlabel[currlink[1][2]])\n",
    "                      \n",
    "                    #if cind == dbline:\n",
    "#                         print('Initial:')\n",
    "#                         print('leftfeat = ', leftfeat)\n",
    "#                         print('rightfeat = ', rightfeat)\n",
    "\n",
    "                    # Handle the wildcard features\n",
    "                    for find, currleft in enumerate(leftfeat):\n",
    "                        if currleft == 0.5:\n",
    "                            leftfeat[find] = rightfeat[find]\n",
    "                        if rightfeat[find] == 0.5:\n",
    "                            rightfeat[find] = leftfeat[find]\n",
    "                            \n",
    "#                     if cind == dbline:\n",
    "#                         print('Revised:')\n",
    "#                         print('leftfeat = ', leftfeat)\n",
    "#                         print('rightfeat = ', rightfeat)            \n",
    "                              \n",
    "                    currmult = np.absolute(np.inner(rho(leftfeat), rho(rightfeat)))/len(leftfeat)\n",
    "    \n",
    "                hl *= currmult\n",
    "  \n",
    "                ## Debugging\n",
    "\n",
    "                if cind == dbline:\n",
    "                    print(hl)\n",
    "                    import pdb; pdb.set_trace()\n",
    "                \n",
    "        hlist.append(hl)\n",
    "    \n",
    "    return hlist\n",
    "    \n",
    "hlist01 = get_local_harmonies(allowed01, auglinklist01, lexicon01)\n",
    "df['Local-Harmonies'] = np.round(hlist01, 6)\n",
    "print('Table 3.  Harmonies associated with the link configurations in Table 2.')\n",
    "# df  ## Print whole table\n",
    "df.iloc[:, [0, 1, 2, 3, 11]]   ## Print without the links to the null node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 4, -1], [1, 1, 0]],\n",
       " [[0, 4, -1], [2, 0, 0]],\n",
       " [[1, 1, -1], [2, 0, 0]],\n",
       " [[1, 1, 2], [2, 0, -1]],\n",
       " [[0, 4, -1], [-2, -2, -2]],\n",
       " [[1, 1, -1], [-2, -2, -2]],\n",
       " [[1, 1, 0], [-2, -2, -2]],\n",
       " [[1, 1, 2], [-2, -2, -2]],\n",
       " [[2, 0, -1], [-2, -2, -2]],\n",
       " [[2, 0, 0], [-2, -2, -2]],\n",
       " [[2, 0, 2], [-2, -2, -2]]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auglinklist01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the global harmony function, HF, out of the local harmony values. In Table 3, we consider all the local harmony values associated with one word string, \"the cats dog\". To build the full harmony function, we must include a term for every local harmony value associated with every possible order of all the words in the vocabulary. That is, we assume different treelets for each word in each position in the sentence, and find the harmonies of all strings with the location-specific treelets. In practice, for modeling comprehension, we will often leave out terms associated with words that do not occur in the sentence. This simplification assumes no confusion of phonological forms and no hallucination or forgetting of words, assumptions which are clearly inaccurate, but which often do not play an important role in particular phenomena of interest. We also sometimes even consider only one order of words (but typically including all initial substrings of that order). This simplification assumes no word-order confusions, again, not a realistic assumption but, nevertheless, one that sometimes does not plausibly affect a particular case of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global harmony function, $HF(\\mathbf{s})$, is constructed by placing an RBF, $\\phi_i$ at each local harmony locus, $h_i$:\n",
    "\n",
    "$$HF(\\mathbf{s}) = \\sum_{i \\in Configurations} h_i\\ \\phi_i(\\mathbf{s})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\phi_i(\\mathbf{s}) = \\exp{\\left(-\\frac{\\Vert\\mathbf{s} - \\mathbf{c}_i\\Vert^2_2}{\\gamma}\\right)}$$\n",
    "\n",
    "Here, $\\mathbf{s}$ is the system state, i.e., the vector of activations of all the links and features. Each of these values ranges, in principle, over $(-\\infty, \\infty)$, but because of the design of the harmony function, the values will stay close the interval $[0, 1]$.  $\\mathbf{c}_i$ is the center of the $i$th RBF. $\\gamma$ is a free parameter which specifies the widths of the RBFs, and thus determines how strongly the system prefers to achieve optimal parses (greater $\\gamma$ means stronger preference for maximal harmony). $\\Vert\\cdot\\Vert^2_2$ is Euclidean distance or the squared L2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of the system are given by the gradient of harmony + noise:\n",
    "\n",
    "$$\\dot{\\mathbf{s}} = \\frac{d}{dt}\\mathbf{s} = \\nabla HF = -\\frac{2}{\\gamma}\\sum_{i} h_i\\ (\\mathbf{s} - \\mathbf{c}_i) \\cdot \\phi_i(\\mathbf{s})\\ + \\ \\alpha\\cdot\\eta$$\n",
    "\n",
    "where $\\dot{\\mathbf{s}}$ denotes the change in the state variable with respect to time and $\\eta$ is normally distributed random noise. The magnitude, $\\alpha$, of the noise is a free parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and running a simulation:  encoding interference example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barker et al. (2001) found that semantic similarity enhanced attraction:\n",
    "\n",
    "    The canoe near the sailboats ...   (MORE plural)\n",
    "    The canoe near the cabins ...      (LESS plural)\n",
    "    \n",
    "This can be understood as an encoding- as opposed to a retrieval-interference effect because (as we far as we know) the verbs do not select for the semantic features (e.g. + Building, + Boat, etc) \n",
    "\n",
    "Villata et al. and Smith et al find, roughly, semantic that similarity in these kinds of sentences also slows down reading time at the verb and causes worse performance on comprehension questions which check to see whether the participants have correctly assigned the semantic roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make a lexicon suitable to this case:\n",
    "\n",
    "    Features:     Noun Verb Prep Boat Building Sg Pl  \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Build the lexicon \n",
    "sim1_canoe =     LexEntry('canoe',  [1, 0, 0, 1, 0, 1, 0],    [(0, 0, 0, 0, 0, 0, 0), \n",
    "                                                               (0, 0, 1, 0, 0, 0, 0)], 0, [], [0, 1], ['PO',\n",
    "                                                                                                       'PP'])\n",
    "sim1_canoes =    LexEntry('canoes', [1, 0, 0, 1, 0, 0, 1],    [(0, 0, 0, 0, 0, 0, 0), \n",
    "                                                               (0, 0, 1, 0, 0, 0, 0)], 0, [], [0, 1], ['PO',\n",
    "                                                                                                       'PP'])\n",
    "sim1_cabin =     LexEntry('cabin',  [1, 0, 0, 0, 1, 1, 0],    [(0, 0, 0, 0, 0, 0, 0), \n",
    "                                                               (0, 0, 1, 0, 0, 0, 0)], 0, [], [0, 1], ['PO',\n",
    "                                                                                                       'PP'])\n",
    "sim1_cabins =    LexEntry('cabins', [1, 0, 0, 0, 1, 0, 1],    [(0, 0, 0, 0, 0, 0, 0), \n",
    "                                                               (0, 0, 1, 0, 0, 0, 0)], 0, [], [0, 1], ['PO',\n",
    "                                                                                                       'PP'])\n",
    "sim1_sinks =     LexEntry('sinks',  [0, 1, 0, 0, 0, 1, 0],    [(1, 0, 0, 0.5, 0.5, 1, 0),\n",
    "                                                               (0, 0, 0, 0,   0,   0, 0)], 1, [], [0, 0], ['NP(Subj)',\n",
    "                                                                                                           'PO'])\n",
    "sim1_sink =      LexEntry('sink',   [0, 1, 0, 0, 0, 0, 1],    [(1, 0, 0, 0.5, 0.5, 0, 1),\n",
    "                                                               (0, 0, 0, 0,   0,   0, 0)], 1, [], [0, 0], ['NP(Subj)',\n",
    "                                                                                                           'PO'])\n",
    "sim1_near =      LexEntry('near',   [0, 0, 1, 0, 0, 0, 0],    [(0, 0, 0, 0,   0,   0,   0),\n",
    "                                                               (1, 0, 0, 0.5, 0.5, 0.5, 0.5)], 0, [], [0, 0], ['PO',\n",
    "                                                                                                            'NP(Obj)'])\n",
    "sim1_Root =      LexEntry('Root',   [],                       [(0, 1, 0, 0, 0, 0.5, 0.5)], 2, [], [0],     ['VP'])  \n",
    "                                                                                                # Head pos = 2\n",
    "                                                                                                # implies VP daughter\n",
    "                                                                                                # is leftward attaching\n",
    "\n",
    "sim1lex = [sim1_canoe, sim1_canoes, sim1_cabin, sim1_cabins, sim1_sinks, sim1_sink, sim1_near, sim1_Root]\n",
    "featurecountsim1 = np.shape(sim1lex[0].dlist)[1]\n",
    "\n",
    "## Add the order info based on the position of the head\n",
    "for ind, word in enumerate(sim1lex):\n",
    "    word.ddir = np.sign(np.array(range(len(word.dopt))) - word.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in a full construction would be to build all strings of all finite lengths from the vocabulary ($\\Sigma^*$, where $\\Sigma$ is the set of vocabulary items).  Since this is impractical, we select a subset that are arguably all the ones relevant to the cases of current interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim1strings = [ # Grammatical:\n",
    "                'canoe near canoe sinks Root', 'canoe near canoes sinks Root', 'canoes near canoe sink Root',\n",
    "                'canoes near canoes sink Root',\n",
    "                'canoe near cabin sinks Root', 'canoe near cabins sinks Root', 'canoes near cabin sink Root',\n",
    "                'canoes near cabins sink Root',\n",
    "                # Ungrammatical:\n",
    "                'canoe near canoe sink Root', 'canoe near canoes sink Root', 'canoes near canoe sinks Root',\n",
    "                'canoes near canoes sinks Root', \n",
    "                'canoe near cabin sink Root', 'canoe near cabins sink Root', 'canoes near cabin sinks Root',\n",
    "                'canoes near cabins sinks Root']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we identify local Harmony maxima for each string, forming htable, a table that can be used to calculate harmonies of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canoe near canoe sinks Root',\n",
       " 'canoe near canoes sinks Root',\n",
       " 'canoes near canoe sink Root',\n",
       " 'canoes near canoes sink Root',\n",
       " 'canoe near cabin sinks Root',\n",
       " 'canoe near cabins sinks Root',\n",
       " 'canoes near cabin sink Root',\n",
       " 'canoes near cabins sink Root']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim1strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of main links:   51\n",
      "Table 4.  A piece of the combined harmony table.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "nullmult = 0.5;  # This is a key parameter:  the harmony multiplier associated with each mandatory \n",
    "                 # attachment site that fails to attach\n",
    "\n",
    "# Find all the links, nodes, and local Harmony maxima for each string\n",
    "complistsim1 = []\n",
    "nlocalh = 0\n",
    "for currstring in sim1strings:\n",
    "    wordlist = get_words(currstring)\n",
    "    namelist = list(x.name for x in sim1lex)\n",
    "    currtlinds = get_str_inds(wordlist, namelist)\n",
    "    currlinklist = get_links(currstring, sim1lex)\n",
    "    actslinks = allowed_configs(currlinklist, sim1lex, currtlinds)\n",
    "    currallowed = actslinks[0]\n",
    "    currauglinklist = actslinks[1]\n",
    "    currauglinklabels = make_labels(currauglinklist, sim1lex, currtlinds)\n",
    "    # Separate nullsite links out\n",
    "    seconds = [x[1] for x in currauglinklist]\n",
    "    nullindex = find_in_list(nullsite, seconds)\n",
    "    currlinksmain = currauglinklist[0:nullindex]\n",
    "    currlinksnull = currauglinklist[nullindex:(len(currauglinklist)+1)]\n",
    "    currlabelsmain = currauglinklabels[0:nullindex]\n",
    "    currlabelsnull = currauglinklabels[nullindex:(len(currauglinklist)+1)]\n",
    "    \n",
    "    currnodelist = get_unique_nodes(currauglinklist)\n",
    "    currhlist = get_local_harmonies(currallowed, currauglinklist, sim1lex)\n",
    "    \n",
    "    complistsim1.append([currallowed, currhlist, currlinksmain, currlabelsmain, currlinksnull, currlabelsnull, currnodelist])\n",
    "    nlocalh += len(currhlist)\n",
    "    \n",
    "# Compile the links and nodes across strings to make the state vector\n",
    "\n",
    "linklistsim1 = []\n",
    "linklabelssim1 = []\n",
    "nodelistsim1 = []\n",
    "nodelabelssim1 = []\n",
    "\n",
    "# Compile main links\n",
    "for sind, currpac in enumerate(complistsim1):\n",
    "    for lind, currlink in enumerate(currpac[2]):\n",
    "        if currlink not in linklistsim1:\n",
    "            linklistsim1.append(currlink)\n",
    "            linklabelssim1.append(currpac[3][lind])\n",
    "nmainlinks = len(linklistsim1)\n",
    "# Add in the null links\n",
    "for sind, currpac in enumerate(complistsim1):\n",
    "    for lind, currlink in enumerate(currpac[4]):\n",
    "        if currlink not in linklistsim1:\n",
    "            linklistsim1.append(currlink)\n",
    "            linklabelssim1.append(currpac[5][lind])\n",
    "# Add in the nodes\n",
    "for sind, currpac in enumerate(complistsim1):\n",
    "    for nind, currnode in enumerate(currpac[6]):\n",
    "        if currnode not in nodelistsim1:\n",
    "            nodelistsim1.append(currnode)\n",
    "            if currnode[2] == -1:\n",
    "                sublabel = 'mother'\n",
    "            else:\n",
    "                sublabel = sim1lex[currnode[1]].dlabel[currnode[2]]\n",
    "            currnodelabel = str(currnode[0]) + '.' + sim1lex[currnode[1]].name + '(' + sublabel + ')'\n",
    "            nodelabelssim1.append(currnodelabel)\n",
    "nullind = find_in_list(nullsite, nodelistsim1)\n",
    "nulllabel = nodelabelssim1[nullind]\n",
    "nodelistsim1.remove(nullsite)\n",
    "nodelabelssim1.remove(nulllabel)\n",
    "\n",
    "\n",
    "\n",
    "## Make labels for the individual features on the nodes of the treelets\n",
    "lastlinkind = len(linklistsim1)-1\n",
    "nstatevars = lastlinkind + 1 + len(nodelistsim1)*featurecountsim1\n",
    "htable = np.zeros((nlocalh, nstatevars))\n",
    "hvals = np.zeros(nlocalh)\n",
    "hlabels = []\n",
    "for llabel in linklabelssim1:\n",
    "    hlabels.append(llabel)\n",
    "for nlabel in nodelabelssim1:\n",
    "    for rind in range(featurecountsim1):\n",
    "         hlabels.append(nlabel)\n",
    "hlabels_arr = np.array(hlabels)\n",
    "                                  \n",
    "# For each local Harmony maximum, make an entry in htable that specifies its state and the local Harmony elevation\n",
    "currlh = -1\n",
    "for pind, currpac in enumerate(complistsim1):  # Go through the strings\n",
    "    for cind, config in enumerate(currpac[0]):  # Go through the maxima for each string\n",
    "        currlh += 1\n",
    "        for lind, lact in enumerate(config):  # Specify activated links\n",
    "            if lact:\n",
    "                if lind < len(currpac[2]):  \n",
    "                    currlink = currpac[2][lind]  # First draw from the main link list\n",
    "                else:\n",
    "                    currlink = currpac[4][lind - len(currpac[2])] # Then draw from the null link list\n",
    "                    \n",
    "                # Activate the link\n",
    "                htable[currlh, find_in_list(currlink, linklistsim1)] = 1\n",
    "                       \n",
    "                # Activate the first attachment site\n",
    "                firstnode = currlink[0]\n",
    "                firstind = find_in_list(firstnode, nodelistsim1)\n",
    "                if firstnode[2] == -1:\n",
    "                       fvals = sim1lex[firstnode[1]].mother\n",
    "                else:\n",
    "                       fvals = sim1lex[firstnode[1]].dlist[firstnode[2]]\n",
    "                bankstart = lastlinkind + (firstind-1)*featurecountsim1 + 1\n",
    "                bankend = lastlinkind + (firstind-1)*featurecountsim1 + featurecountsim1 + 1\n",
    "                htable[currlh, bankstart:bankend] = fvals\n",
    "\n",
    "                # Activate the second attachment site\n",
    "                secondnode = currlink[1]\n",
    "                if secondnode[0] != -2: # No need to activate features for the null site\n",
    "                    secondind = find_in_list(secondnode, nodelistsim1)\n",
    "                    if secondnode[2] == -1:\n",
    "                        fvals = sim1lex[secondnode[1]].mother\n",
    "                    else:\n",
    "                        fvals = sim1lex[secondnode[1]].dlist[secondnode[2]]\n",
    "                    bankstart = lastlinkind + (secondind-1)*featurecountsim1 + 1\n",
    "                    bankend = lastlinkind + (secondind-1)*featurecountsim1 + featurecountsim1 + 1\n",
    "                    htable[currlh, bankstart:bankend] = fvals\n",
    "        hvals[currlh] = currpac[1][cind] \n",
    "        \n",
    "#         if (pind == 5) & (cind == 59):\n",
    "#             print(hvals[currlh])\n",
    "#             pdb.set_trace()\n",
    "        \n",
    "#         if (currlh == 12) or (currlh == 828):\n",
    "#             print(currlh)\n",
    "#             pdb.set_trace()\n",
    "        \n",
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "# Reduce the harmony table by removing very low harmony peaks\n",
    "hmin = 0.2\n",
    "#hmin = 0.12\n",
    "#hmin = 1   # Try including only grammatical structures\n",
    "\n",
    "rhtable = []\n",
    "rhvals = []\n",
    "rtohind = []\n",
    "for hind, currhval in enumerate(hvals):\n",
    "    if currhval >= hmin:\n",
    "        rhtable.append(htable[hind, :])\n",
    "        rhvals.append(currhval)\n",
    "        rtohind.append(hind)\n",
    "rhtable = np.array(rhtable)\n",
    "rhvals = np.array(rhvals)\n",
    "rtohind = np.array(rtohind)\n",
    "\n",
    "# Kludge high harmony for 41\n",
    "#rhvals[41] = 2.0\n",
    "#rhvals[41] = 0.5\n",
    "\n",
    "rhmain = rhtable[:, 0:nmainlinks] # Store this useful subpart for reference later\n",
    "\n",
    "\n",
    "dfallh = pd.DataFrame(rhtable, index=None, columns = hlabels)\n",
    "dfallh['Local-Harmonies'] = rhvals\n",
    "# print('Table 4.  Combined harmony table.')\n",
    "# dfallh  ## Print whole table\n",
    "\n",
    "#dfallh.iloc[0:4, np.concatenate(np.arange(0, 16), [np.shape(dfallh)[1] - 1])]  # Print a small piece\n",
    "#dfallh.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, np.shape(dfallh)[1] - 1]]\n",
    "print('Number of main links:  ', nmainlinks)\n",
    "print('Table 4.  A piece of the combined harmony table.')\n",
    "#dfallh.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, np.shape(dfallh)[1] - 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 211)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(rhtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog near dogs sleep Root\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some probing\n",
    "\n",
    "print(sim1strings[5])\n",
    "\n",
    "\n",
    "currpac5 = complistsim1[5]\n",
    "nmainlinks5 = len(currpac5[3])\n",
    "\n",
    "mainlabels5 = currpac5[3]\n",
    "mainlabels5\n",
    "\n",
    "mainlabels5 = np.array(mainlabels5)\n",
    "\n",
    "targmain5 = np.zeros(nmainlinks5)\n",
    "tvec = [2, 9, 12]\n",
    "targmain5[np.array(tvec)] = 1\n",
    "\n",
    "configs = currpac5[0]\n",
    "configs = configs[:, 0:nmainlinks5]\n",
    "\n",
    "target = targmain5\n",
    "#target = np.array(configs[3])\n",
    "\n",
    "matchlist = []\n",
    "for hind, config in enumerate(configs):\n",
    "    if (config == target).all():\n",
    "        matchlist.append(hind)\n",
    "matchlist\n",
    "\n",
    "#configs[59]\n",
    "\n",
    "#mainlabels5[tvec]\n",
    "\n",
    "hlist5 = currpac5[1]\n",
    "hlist5[59]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activated at peak 12:   [ 7 12 35 36 37 40 42 46 49 51 54 57 63 68 71 74 77 79 81 88]\n",
      "activated at peak 828:   [  7  12  37  40  42  47  48  51  54  57  68  71  74  77  79  81  88 106\n",
      " 110 118]\n",
      "['1.near(NP(Obj))-2.dog(mother)' '3.sleeps(mother)-4.Root(VP)']\n",
      "main links shape:   (13, 2, 3)\n",
      "null links shape:   (10, 2, 3)\n",
      "config of peak 12:   [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.\n",
      "  1.  0.  1.  0.  0.]\n",
      "labels for main links of 12:   ['0.dog(mother)-3.sleeps(NP(Subj))', '0.dog(mother)-4.Root(VP)', '0.dog(PP)-1.near(mother)', '0.dog(PP)-2.dog(mother)', '0.dog(PP)-3.sleeps(mother)', '1.near(mother)-3.sleeps(NP(Subj))', '1.near(mother)-4.Root(VP)', '1.near(NP(Obj))-2.dog(mother)', '1.near(NP(Obj))-3.sleeps(mother)', '2.dog(mother)-3.sleeps(NP(Subj))', '2.dog(mother)-4.Root(VP)', '2.dog(PP)-3.sleeps(mother)', '3.sleeps(mother)-4.Root(VP)']\n",
      "labels for null links of 12:   ['0.dog(mother)-Null(Null)', '0.dog(PP)-Null(Null)', '1.near(mother)-Null(Null)', '1.near(NP(Obj))-Null(Null)', '2.dog(mother)-Null(Null)', '2.dog(PP)-Null(Null)', '3.sleeps(mother)-Null(Null)', '3.sleeps(NP(Subj))-Null(Null)', '4.Root(mother)-Null(Null)', '4.Root(VP)-Null(Null)']\n",
      "main links shape:   (13, 2, 3)\n",
      "null links shape:   (10, 2, 3)\n",
      "config of peak 12:   [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.\n",
      "  1.  0.  1.  0.  0.]\n",
      "labels for main links of 12:   ['0.dogs(mother)-3.sleeps(NP(Subj))', '0.dogs(mother)-4.Root(VP)', '0.dogs(PP)-1.near(mother)', '0.dogs(PP)-2.dog(mother)', '0.dogs(PP)-3.sleeps(mother)', '1.near(mother)-3.sleeps(NP(Subj))', '1.near(mother)-4.Root(VP)', '1.near(NP(Obj))-2.dog(mother)', '1.near(NP(Obj))-3.sleeps(mother)', '2.dog(mother)-3.sleeps(NP(Subj))', '2.dog(mother)-4.Root(VP)', '2.dog(PP)-3.sleeps(mother)', '3.sleeps(mother)-4.Root(VP)']\n",
      "labels for null links of 12:   ['0.dogs(mother)-Null(Null)', '0.dogs(PP)-Null(Null)', '1.near(mother)-Null(Null)', '1.near(NP(Obj))-Null(Null)', '2.dog(mother)-Null(Null)', '2.dog(PP)-Null(Null)', '3.sleeps(mother)-Null(Null)', '3.sleeps(NP(Subj))-Null(Null)', '4.Root(mother)-Null(Null)', '4.Root(VP)-Null(Null)']\n"
     ]
    }
   ],
   "source": [
    "## Some probing\n",
    "\n",
    "#print(rtohind[1])  # Result:  12\n",
    "#print(rtohind[45])  # Result:  828\n",
    "print('activated at peak 12:  ', np.where(htable[12, :] == 1)[0])\n",
    "print('activated at peak 828:  ', np.where(htable[828, :] == 1)[0])  # complistsim1[6] item 12, I think\n",
    "\n",
    "print(hlabels_arr[np.array([7, 12])])\n",
    "nmainlinks, lastlinkind\n",
    "\n",
    "# for pind, currpac in enumerate(complistsim1):  # Go through the strings\n",
    "#     print(pind, np.shape(currpac[0]))\n",
    "\n",
    "currpac0 = complistsim1[0]\n",
    "\n",
    "print('main links shape:  ', np.shape(currpac0[2]))\n",
    "print('null links shape:  ', np.shape(currpac0[4]))\n",
    "\n",
    "print('config of peak 12:  ', currpac0[0][12])\n",
    "print('labels for main links of 12:  ', currpac0[3])\n",
    "print('labels for null links of 12:  ', currpac0[5])\n",
    "\n",
    "currpac6 = complistsim1[6]\n",
    "\n",
    "print('main links shape:  ', np.shape(currpac6[2]))\n",
    "print('null links shape:  ', np.shape(currpac6[4]))\n",
    "\n",
    "print('config of peak 12:  ', currpac6[0][12])\n",
    "print('labels for main links of 12:  ', currpac6[3])\n",
    "print('labels for null links of 12:  ', currpac6[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 131) 35 50\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwN52P4//kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c4ddf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median hval =  0.008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Histogram of harmonies\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "print(np.shape(rhtable), nmainlinks, lastlinkind)\n",
    "# print(np.where(hlabels_arr == '0.dog(mother)')[0]) \n",
    "# print(np.where(hlabels_arr == '4.Root(VP)')[0]) \n",
    "# hlabels[51:57]\n",
    "#print(hlabels[0:nmainlinks])\n",
    "\n",
    "pl.hist(np.around(hvals, 4), bins='auto')\n",
    "pl.title('Histogram of harmony values, rounded')\n",
    "pl.show()\n",
    "\n",
    "print('median hval = ', np.median(hvals))\n",
    "len(np.where(hvals >= 0.12)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration, run the network once with the input \"dog near dogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected harmony peak:   6\n",
      "harmony of peak:   1.0\n",
      "distance from peak:   1.99276563116\n",
      "number of steps:   56\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "# maxsteps = 200    # Maximum number of steps per settling episode\n",
    "# gamma = 1        # Width parameter for Gaussians\n",
    "# #gamma = 0.5\n",
    "# #gamma = 0.25\n",
    "# #gamma = 1\n",
    "# #tol = 0.1;         # Distance from goal that counts as having successfully reached it\n",
    "# #tol = 0.2;\n",
    "# tol = 1\n",
    "# #tol = 3\n",
    "# #tol = 0.5\n",
    "# #noisemag = 0.05;   # Magnitude of gaussian random noise applied at very activation update (Euler integration)\n",
    "# #noisemag = 0.0001;\n",
    "# #noisemag = 0.005; # Works with gamma = 1\n",
    "# noisemag = 0.05; # \n",
    "# #noisemag = 0.1; # Shows some variety (tol = 3, gamma = 0.25) including with 'dog near dogs'\n",
    "# #nflips = 5;  # \n",
    "# stepcontract = 5;\n",
    "\n",
    "maxsteps = 200    # Maximum number of steps per settling episode\n",
    "gamma = 2        # Width parameter for Gaussians\n",
    "tol = 2\n",
    "noisemag = 0.06;\n",
    "stepcontract = 5;\n",
    "\n",
    "def getda(aa, htable, hvals, gamma):\n",
    "\n",
    "    da = np.zeros(len(aa))\n",
    "    \n",
    "    currmin = 4*len(aa)\n",
    "    for hind, currh in enumerate(hvals):\n",
    "        currdiffvec = aa - htable[hind, :]\n",
    "        currmag = np.linalg.norm(currdiffvec)\n",
    "        phi = np.exp(-currmag**2/gamma)\n",
    "        da = da + hvals[hind] * phi * currdiffvec\n",
    "        \n",
    "        #if hind == 41:\n",
    "        #   pdb.set_trace()\n",
    "        \n",
    "        if currmag < currmin:\n",
    "            currmin = currmag\n",
    "        \n",
    "    da = -2 * da / gamma\n",
    "    \n",
    "    return (da, currmin)\n",
    "\n",
    "def gravitate(aa0, rhtable, rhvals, gamma, maxsteps, stepcontract):\n",
    "    \n",
    "    # The novelty of this version as that noise works by flipping bits\n",
    "\n",
    "    aa = deepcopy(aa0)\n",
    "    stable = 0\n",
    "    scount = 0\n",
    "    ehist = []\n",
    "    while (not stable) & (scount < maxsteps):\n",
    "        scount += 1\n",
    "        dapac = getda(aa, rhtable, rhvals, gamma)\n",
    "\n",
    "        da = dapac[0]\n",
    "\n",
    "        stepsize = 1 / (stepcontract*np.linalg.norm(da))\n",
    "        aa += stepsize*da + noisemag*np.random.normal(0, 1, ndim)\n",
    "        #aa += stepsize*da\n",
    "        #flipcan = np.random.randint(0, ndim-1, nflips)\n",
    "        \n",
    "#         for dind, cind in enumerate(flipcan):\n",
    "#             if np.random.randint(0, 2) == 1:\n",
    "#                 aa[dind] = 1 - aa[dind]\n",
    "\n",
    "        #stable = np.linalg.norm(da) < tol \n",
    "        ehist.append(dapac[1])\n",
    "        stable = dapac[1] < tol\n",
    "        \n",
    "    return (aa, ehist)\n",
    "\n",
    "def set_initial_state(currstring, currlex, ndim, grading):\n",
    "\n",
    "    aa0 = np.zeros(ndim)\n",
    "    \n",
    "    wordstring = get_words(currstring)\n",
    "    namelist = list(x.name for x in sim1lex)\n",
    "    tlindssim1 = get_str_inds(wordstring, namelist)\n",
    "    if len(grading) == 0:\n",
    "        grading = np.ones(len(wordstring))\n",
    "    for wind, currword in enumerate(wordstring):\n",
    "        currlexind = tlindssim1[wind]\n",
    "        if currlexind > -1:   # If there is no word in the current position, then make no change based on the position\n",
    "            curritem = sim1lex[currlexind]\n",
    "            if len(curritem.mother) > 0:\n",
    "                currlabel = str(wind) + '.' + currword + '(' + 'mother' + ')'\n",
    "                currinds = np.where(hlabels_arr == currlabel)[0]\n",
    "                aa0[currinds] = grading[wind]*curritem.mother\n",
    "            for dcount, optval in enumerate(curritem.dopt):\n",
    "                if ~optval & (not dcount == curritem.head):\n",
    "                    currlabel = str(wind) + '.' + currword + '(' + curritem.dlabel[dcount] + ')'\n",
    "                    currinds = np.where(hlabels_arr == currlabel)[0]\n",
    "                    aa0[currinds] = grading[wind]*curritem.dlist[dcount]\n",
    "                    if len(currinds) == 0:\n",
    "                        print('Problem:  label not found: ', currlabel)\n",
    "                        pdb.set_trace()\n",
    "    return aa0\n",
    "\n",
    "def get_choice(aa, htable):\n",
    "\n",
    "    dvec = []\n",
    "    for fvec in htable:\n",
    "        dvec.append(np.linalg.norm(aa - fvec))\n",
    "    mindist = np.min(dvec)\n",
    "    currchoice = np.where(dvec == mindist)[0]\n",
    "    currchoice = currchoice[0]  ###  Note:   I don't know why this is necessary!! (Glosses over possib. of mult. mins)\n",
    "\n",
    "    return currchoice, mindist\n",
    "\n",
    "### Main\n",
    "\n",
    "## In this version, set all the feature vectors for the words at once (rather than presenting them sequentially)\n",
    "## Set each feature vector to \n",
    "\n",
    "ndim = np.shape(htable)[1]\n",
    "\n",
    "currstring = 'canoe near cabin'; grading = np.array([])\n",
    "\n",
    "# grading = []\n",
    "aa0 = set_initial_state(currstring, sim1lex, ndim, grading)  # Set grading to empty for no grading\n",
    "\n",
    "# Kludge initial link activations\n",
    "#aa0[0:lastlinkind] = 0.01*rhtable[41, 0:lastlinkind]\n",
    "\n",
    "(aa, ehist) = gravitate(aa0, rhtable, rhvals, gamma, maxsteps, stepcontract) \n",
    "    \n",
    "aaround = np.around(aa, 2)\n",
    "aaroundmain = aaround[0:nmainlinks]\n",
    "\n",
    "(currchoice, mindist) = get_choice(aa, rhtable)\n",
    "    \n",
    "print('selected harmony peak:  ', currchoice)\n",
    "print('harmony of peak:  ', rhvals[currchoice])\n",
    "print('distance from peak:  ', mindist)\n",
    "print('number of steps:  ', len(ehist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastlinkind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypeak =  35\n",
      "hval =  1.0\n",
      "keyind =  [14 18 20 27]\n",
      "['1.near(NP(Obj))-2.dogs(mother)' '0.dogs(mother)-3.sleep(NP(Subj))'\n",
      " '0.dogs(PP)-1.near(mother)' '3.sleep(mother)-4.Root(VP)']\n",
      "keypeak =  17\n",
      "hval =  1.0\n",
      "keyind =  [ 0  2 12 14]\n",
      "['0.dog(mother)-3.sleeps(NP(Subj))' '0.dog(PP)-1.near(mother)'\n",
      " '3.sleeps(mother)-4.Root(VP)' '1.near(NP(Obj))-2.dogs(mother)']\n"
     ]
    }
   ],
   "source": [
    "keypeak = 35\n",
    "print(\"keypeak = \", keypeak)\n",
    "print(\"hval = \", rhvals[keypeak])\n",
    "keyind = np.where(rhmain[keypeak, :] == 1)[0]\n",
    "print(\"keyind = \", keyind)\n",
    "print(hlabels_arr[keyind])\n",
    "\n",
    "keypeak = 17\n",
    "print(\"keypeak = \", keypeak)\n",
    "print(\"hval = \", rhvals[keypeak])\n",
    "keyind = np.where(rhmain[keypeak, :] == 1)[0]\n",
    "print(\"keyind = \", keyind)\n",
    "print(hlabels_arr[keyind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuYuIFKH/A+wYHRrxaE4dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111e78240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "pl.plot(range(len(ehist)), ehist, 'r')\n",
    "pl.title('Distance from nearest peak vs. time')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.dog(mother)-3.sleeps(NP(Subj))' '0.dog(mother)-4.Root(VP)'\n",
      " '0.dog(PP)-1.near(mother)' '0.dog(PP)-2.dog(mother)'\n",
      " '0.dog(PP)-3.sleeps(mother)' '1.near(mother)-3.sleeps(NP(Subj))'\n",
      " '1.near(mother)-4.Root(VP)' '1.near(NP(Obj))-2.dog(mother)'\n",
      " '1.near(NP(Obj))-3.sleeps(mother)' '2.dog(mother)-3.sleeps(NP(Subj))'\n",
      " '2.dog(mother)-4.Root(VP)' '2.dog(PP)-3.sleeps(mother)'\n",
      " '3.sleeps(mother)-4.Root(VP)' '0.dog(PP)-2.dogs(mother)'\n",
      " '1.near(NP(Obj))-2.dogs(mother)' '2.dogs(mother)-3.sleeps(NP(Subj))'\n",
      " '2.dogs(mother)-4.Root(VP)' '2.dogs(PP)-3.sleeps(mother)'\n",
      " '0.dogs(mother)-3.sleep(NP(Subj))' '0.dogs(mother)-4.Root(VP)'\n",
      " '0.dogs(PP)-1.near(mother)' '0.dogs(PP)-2.dog(mother)'\n",
      " '0.dogs(PP)-3.sleep(mother)' '1.near(mother)-3.sleep(NP(Subj))'\n",
      " '1.near(NP(Obj))-3.sleep(mother)' '2.dog(mother)-3.sleep(NP(Subj))'\n",
      " '2.dog(PP)-3.sleep(mother)' '3.sleep(mother)-4.Root(VP)'\n",
      " '0.dogs(PP)-2.dogs(mother)' '2.dogs(mother)-3.sleep(NP(Subj))'\n",
      " '2.dogs(PP)-3.sleep(mother)' '0.dog(mother)-3.sleep(NP(Subj))'\n",
      " '0.dog(PP)-3.sleep(mother)' '0.dogs(mother)-3.sleeps(NP(Subj))'\n",
      " '0.dogs(PP)-3.sleeps(mother)']\n"
     ]
    }
   ],
   "source": [
    "print(hlabels_arr[0:nmainlinks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the full simulation:  repeatedly present critical sentences to the network, reporting the distribution over final states for each form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of  1000 :  [ 0 ][ 10 ][ 20 ][ 30 ][ 40 ][ 50 ][ 60 ][ 70 ][ 80 ][ 90 ][ 100 ]\n",
      "[ 110 ][ 120 ][ 130 ][ 140 ][ 150 ][ 160 ][ 170 ][ 180 ][ 190 ][ 200 ]\n",
      "[ 210 ][ 220 ][ 230 ][ 240 ][ 250 ][ 260 ][ 270 ][ 280 ][ 290 ][ 300 ]\n",
      "[ 310 ][ 320 ][ 330 ][ 340 ][ 350 ][ 360 ][ 370 ][ 380 ][ 390 ][ 400 ]\n",
      "[ 410 ][ 420 ][ 430 ][ 440 ][ 450 ][ 460 ][ 470 ][ 480 ][ 490 ][ 500 ]\n",
      "[ 510 ][ 520 ][ 530 ][ 540 ][ 550 ][ 560 ][ 570 ][ 580 ][ 590 ][ 600 ]\n",
      "[ 610 ][ 620 ][ 630 ][ 640 ][ 650 ][ 660 ][ 670 ][ 680 ][ 690 ][ 700 ]\n",
      "[ 710 ][ 720 ][ 730 ][ 740 ][ 750 ][ 760 ][ 770 ][ 780 ][ 790 ][ 800 ]\n",
      "[ 810 ][ 820 ][ 830 ][ 840 ][ 850 ][ 860 ][ 870 ][ 880 ][ 890 ][ 900 ]\n",
      "[ 910 ][ 920 ][ 930 ][ 940 ][ 950 ][ 960 ][ 970 ][ 980 ][ 990 ]  Done\n",
      "canoe near cabin\n",
      "   Singular  Plural  Both  Neither\n",
      "0     0.921   0.079   0.0      0.0\n",
      "    Singular     Plural  Both  Neither\n",
      "0  67.184582  69.341772   0.0      0.0\n"
     ]
    }
   ],
   "source": [
    "def num_indicators(hlabels, nmainlinks):\n",
    "\n",
    "    singstring = '3.sinks(mother)'\n",
    "    pluralstring = '3.sink(mother)'\n",
    "\n",
    "    singindicator = []\n",
    "    pluralindicator = []\n",
    "    for lind, currlabel in enumerate(hlabels[0:nmainlinks]):\n",
    "        if currlabel.find(singstring) == 0:\n",
    "            singindicator.append(lind)\n",
    "        if currlabel.find(pluralstring) == 0:\n",
    "            pluralindicator.append(lind)\n",
    "\n",
    "    if len(singindicator) > 1:\n",
    "        print('Warning:  multiple singular indicators found.')\n",
    "    if len(pluralindicator) > 1:\n",
    "        print('Warning:  multiple plural indicators found.')\n",
    "\n",
    "    return np.array([singindicator[0], pluralindicator[0]])\n",
    "\n",
    "def get_rates_and_times(peaklist, rtlist, htable):\n",
    "\n",
    "    ntab = np.zeros(4)  # 0 = Singular, 1 = Plural, 2 = both, 3 = neither\n",
    "    sumtime = np.zeros(4)\n",
    "    numvals = []\n",
    "    for pind, currpeak in enumerate(peaklist):\n",
    "        currprobe = htable[currpeak, nind]\n",
    "        if (currprobe == np.array([1, 0])).all():\n",
    "            ntab[0] += 1\n",
    "            sumtime[0] += rtlist[pind]\n",
    "            numvals.append(0)\n",
    "        else:\n",
    "            if (currprobe == np.array([0, 1])).all():\n",
    "                ntab[1] += 1\n",
    "                sumtime[1] += rtlist[pind]\n",
    "                numvals.append(1)\n",
    "            else:\n",
    "                if (currprobe == np.array([1, 1])).all():\n",
    "                    ntab[2] += 1\n",
    "                    sumtime[2] += rtlist[pind]\n",
    "                    numvals.append(2)\n",
    "                else:\n",
    "                    ntab[3] += 1\n",
    "                    sumtime[3] += rtlist[pind]\n",
    "                    numvals.append(3)\n",
    "                    \n",
    "    return (ntab, sumtime, numvals)\n",
    "\n",
    "### Main\n",
    "\n",
    "## Set run parameters\n",
    "nruns = 1000\n",
    "\n",
    "maxsteps = 200    # Maximum number of steps per settling episode\n",
    "gamma = 2        # Width parameter for Gaussians\n",
    "tol = 2\n",
    "noisemag = 0.06;\n",
    "stepcontract = 5;\n",
    "\n",
    "## Establish number indicators\n",
    "nind = num_indicators(hlabels, nmainlinks)\n",
    "\n",
    "## Set string to test\n",
    "#currstring = 'canoe near canoe'; grading = np.array([0.25, 0.5, 1]) \n",
    "#currstring = 'canoe near canoes'; grading = np.array([0.25, 0.5, 1])\n",
    "currstring = 'canoe near cabin'; grading = np.array([0.25, 0.5, 1]) \n",
    "#currstring = 'canoe near cabins'; grading = np.array([0.25, 0.5, 1])\n",
    "\n",
    "## Run multiple runs\n",
    "peaklist = []  \n",
    "rtlist = []\n",
    "print('Of ', nruns, ':  ', end=\"\")\n",
    "for rcount in range(nruns):\n",
    "\n",
    "    if np.mod(rcount, 10) == 0:\n",
    "        print('[', rcount, ']', end=\"\"),\n",
    "    if (np.mod(rcount, 100) == 0) & (rcount > 0):\n",
    "        print()\n",
    "        \n",
    "    aa0 = set_initial_state(currstring, sim1lex, ndim, grading)\n",
    "    (aa, ehist) = gravitate(aa0, rhtable, rhvals, gamma, maxsteps, stepcontract)\n",
    "    (currchoice, mindist) = get_choice(aa, rhtable)\n",
    "    \n",
    "    peaklist.append(currchoice)\n",
    "    rtlist.append(len(ehist))\n",
    "    \n",
    "print('  Done')\n",
    "\n",
    "## Determine plural rates                   \n",
    "(ntab, sumtime, numvals) = get_rates_and_times(peaklist, rtlist, rhtable)\n",
    "nprop = ntab/len(peaklist)\n",
    "meantime = np.zeros(4)\n",
    "for tabind, tabval in enumerate(ntab):\n",
    "    if tabval > 0:\n",
    "        meantime[tabind] = sumtime[tabind]/tabval\n",
    "        \n",
    "nproplabels = ['Singular', 'Plural', 'Both', 'Neither']\n",
    "\n",
    "## Display\n",
    "nprop = np.reshape(nprop, (-1, 4))\n",
    "dfnprop = pd.DataFrame(nprop, index=None, columns = nproplabels)\n",
    "print(currstring)\n",
    "print(dfnprop)\n",
    "\n",
    "meantime = np.reshape(meantime, (-1, 4))\n",
    "dfmeantime = pd.DataFrame(meantime, index=None, columns = nproplabels)\n",
    "print(dfmeantime)\n",
    "\n",
    "#(visited, indices) = np.unique(peaklist, return_index=True)\n",
    "#u, indices = np.unique(a, return_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results (1000 runs)\n",
    "\n",
    "canoe near canoe\n",
    "\n",
    "            Singular  Plural  Both  Neither\n",
    "    Rate    0.999     0.001   0.0   0.0\n",
    "    MeanRt  65.92     59.0    0.0   0.0\n",
    "\n",
    "canoe near canoes\n",
    "\n",
    "            Singular  Plural  Both  Neither\n",
    "    Rate    0.92      0.08    0.0   0.0\n",
    "    MeanRt  67.17     70.825  0.0   0.0\n",
    "    \n",
    "canoe near cabin\n",
    "   \n",
    "            Singular  Plural  Both  Neither\n",
    "    Rate    0.921     0.079   0.0   0.0\n",
    "    MeanRT  67.18     69.34   0.0   0.0\n",
    "\n",
    "canoe near cabins\n",
    "\n",
    "            Singular  Plural  Both  Neither\n",
    "    Rate    0.93      0.07    0.0   0.0\n",
    "    MeanRT  67.54     70.1    0.0   0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog near dogs\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, 0, 3, 1, 1, 0, 0, 1, 1, 1, 0, 1, 3, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 3, 0, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "keypeak =  2\n",
      "hval =  1.0\n",
      "keyind =  [ 7 18 20 27]\n",
      "['1.near(NP(Obj))-2.dog(mother)' '0.dogs(mother)-3.sleep(NP(Subj))'\n",
      " '0.dogs(PP)-1.near(mother)' '3.sleep(mother)-4.Root(VP)']\n",
      "keypeak =  3\n",
      "hval =  1.0\n",
      "keyind =  [14 18 20 27]\n",
      "['1.near(NP(Obj))-2.dogs(mother)' '0.dogs(mother)-3.sleep(NP(Subj))'\n",
      " '0.dogs(PP)-1.near(mother)' '3.sleep(mother)-4.Root(VP)']\n"
     ]
    }
   ],
   "source": [
    "print(currstring)\n",
    "print(peaklist)\n",
    "print(numvals)\n",
    "#print(rhvals[peaklist])\n",
    "\n",
    "#np.where(peaklist == 41)[0]\n",
    "\n",
    "# peaklist = np.array(peaklist)\n",
    "\n",
    "# print('harmony = 0.25 run #s:  ', np.where(rhvals[peaklist] == 0.25)[0])\n",
    "# print('harmony = 0.25 htable entries:  ', np.unique(peaklist[np.where(rhvals[peaklist] == 0.25)[0]]))\n",
    "\n",
    "# #keypeak = 35\n",
    "keypeak = 2\n",
    "print('keypeak = ', keypeak)\n",
    "print(\"hval = \", rhvals[keypeak])\n",
    "keyind = np.where(rhmain[keypeak, :] == 1)[0]\n",
    "print(\"keyind = \", keyind)\n",
    "print(hlabels_arr[keyind])\n",
    "\n",
    "keypeak = 3\n",
    "print('keypeak = ', keypeak)\n",
    "print(\"hval = \", rhvals[keypeak])\n",
    "keyind = np.where(rhmain[keypeak, :] == 1)[0]\n",
    "print(\"keyind = \", keyind)\n",
    "print(hlabels_arr[keyind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 35)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(rhmain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the present case, we compared the cases\n",
    "\n",
    "    dog near dogs...\n",
    "    dog near dog...\n",
    "    \n",
    "The distribution of final states for each case is shown in Table 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[93]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Why does the model never pick the locally coherent analysis in \"dog near dogs...\"?\n",
    "\n",
    "#print(hlabels_arr[0:nmainlinks])\n",
    "\n",
    "lcindsother = np.array([2, 27, 29])\n",
    "lcinds = np.array([2, 15, 27])\n",
    "targmain = np.zeros(nmainlinks)\n",
    "targmain[lcindsother] = 1\n",
    "\n",
    "duplinds = np.array([15, 29])  # Not actually duplicated---one has plural sleep, the other sg\n",
    "\n",
    "## double check:\n",
    "# print(hlabels_arr[lcindsother])\n",
    "# print(hlabels_arr[lcinds])\n",
    "# print(hlabels_arr[duplinds])\n",
    "\n",
    "\n",
    "#target = rhmain[1]\n",
    "target = targmain\n",
    "\n",
    "matchlist = []\n",
    "currtable = rhmain\n",
    "#currtable = htable[:, 0:nmainlinks]\n",
    "for hind, config in enumerate(currtable):\n",
    "    if (config == target).all():\n",
    "        matchlist.append(hind)\n",
    "matchlist   # Result (htable):   739   Result (rhmain):  41\n",
    "\n",
    "#(rhtable[1, 0:nmainlinks] == rhtable[45, 0:nmainlinks]).all()     # Result:  True\n",
    "#(rhtable[1, 0:lastlinkind] == rhtable[45, 0:lastlinkind]).all()  # Result:  False\n",
    "#print(rhtable[1, (nmainlinks+1):lastlinkind])\n",
    "#print(rhtable[45, (nmainlinks+1):lastlinkind])\n",
    "\n",
    "#np.shape(np.unique(hlabels_arr[0:nmainlinks]))\n",
    "\n",
    "# np.where(rtohind == 739)[0]\n",
    "# hlabels[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

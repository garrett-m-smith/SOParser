{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An SOSP model of encoding interference in verb choice and reading times\n",
    "For a more complete description of the overall SOSP framework see the ClassicAgreementAttraction notebook. \n",
    "\n",
    "## A case of encoding interference\n",
    "Here, we focus on a case of encoding interference in two sentence processing tasks. Encoding interference is when features of words in a sentence that are not relevant for retrieving those words for structure-building nonetheless interfere with parsing. The example we focus on here builds on the results of Barker, Nicol, & Garrett (2001). They used the sentence completion paradigm common in agreement attraction studies using subject NPs like the following:\n",
    "\n",
    "1. The canoe by the cabin...\n",
    "2. The canoe by the cabins...\n",
    "3. The canoe by the sailboat...\n",
    "4. The canoe by the sailboats...\n",
    "\n",
    "Barker et al. manipulated the number marking on the second noun (N2; *cabin(s)* or *sailboat(s)*) and the semantic similarity between the first noun (N1; *canoe*) and the N2 (canoes and sailboats are both boats, while canoes and cabins have relatively little in common). They found significantly higher rates of agreement attraction in the semantically similar case than in the semantically dissimilar case. This suggests that the semantic similarity was interfering with participants' ability to choose the correct verb form, even though the semantic similarity is not, on most theories, a cue relevant for determining what the controller of verb number should be.\n",
    "\n",
    "Another question is how long it takes to read or produce a verb after reading/producing the subject NP. We assume that doing so puts the parser in a particular state from which it must then settle in order to build the next bit of structure, i.e., incorporate a verb in the the existing parse. Which verb for the model settles on (the Barker et al. production results) and how long that takes (modeling new data we've gathered) are the main goals of this simulation.\n",
    "\n",
    "## A simple SOSP approach to modeling the Barker et al. results\n",
    "To model the Barker et al. results, we make the assumption that the subject NP has already been read/produced. We assume that this has pushed the verb's features toward expecting a the boat-like, singular N1 to be the subject. This is the initial condition for the model to start settling towards a parse. We define the dynamics by setting up a harmony landscape with peaks of different heights corresponding to different parses of varying degrees of wellformedness. We first set up the peaks, which are the centers of the RBFs that make up the global harmony function $HF$. The dimensions code the boat-features on the verb (0 = no boat, 1 = boat), the verb's number feature (0 = sg., 1 = pl.), and the strength of the N2-verb attachment link (we assume that the N1- and N2-verb links are in complementary distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Here, we assume three identical boat features, which weights them quite heavily relative to the single number feature\n",
    "dim_labels = ['boat1', 'boat2', 'boat3', 'pl', 'N2-attach']\n",
    "condition_labels = ['canoe-kayaks', 'canoe-kayak', 'canoe-cabins', 'canoe-cabin']\n",
    "ncond = len(condition_labels)\n",
    "\n",
    "# Dimensions of the state space: [boat1, boat2, boat3, pl, ], +boat=1, +pl=1\n",
    "# These are all of the attractors of the dynamics:\n",
    "centers = np.array([[1.,1, 1, 0, 0],\n",
    "                    [1, 1, 1, 0, 1],\n",
    "                    [0, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 0, 1],\n",
    "                    [0, 0, 0, 1, 0],\n",
    "                    [0, 0, 0, 1, 1],\n",
    "                    [1, 1, 1, 1, 0],\n",
    "                    [1, 1, 1, 1, 1]])\n",
    "    \n",
    "ndim = np.shape(centers)[1]\n",
    "\n",
    "# Setting the initial conditions to reflect the bias N1 has already exerted on the verb, \n",
    "# starting it near the correct attractor\n",
    "x0 = [0.6, 0.6, 0.6, 0.4, 0.4]\n",
    "\n",
    "# We now calculate the heights of the harmony peaks\n",
    "af = 0.8  # cost for failing to attach N1 and attaching N2 instead\n",
    "mf = 0.2  # cost for having a mismatching feature\n",
    "\n",
    "# Harmony values: \n",
    "# Columns are in same order as cond_labels\n",
    "# Rows are in the same order as centers\n",
    "harmony_vals = np.array([[       1,        1,        1,        1],\n",
    "                         [   af*mf,       af, af*mf*mf,    af*mf],\n",
    "                         [      mf,       mf,       mf,       mf],\n",
    "                         [af*mf*mf,    af*mf,    af*mf,       af],\n",
    "                         [   mf*mf,    mf*mf,    mf*mf,    mf*mf],\n",
    "                         [   af*mf, af*mf*mf,       af,    af*mf],\n",
    "                         [      mf,       mf,       mf,       mf],\n",
    "                         [      af,    af*mf,    af*mf, af*mf*mf]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attachment failure and mismatching feature penalties effectively say that the system finds fragmentary speech tolerable and will parse it as such, but it prefers to avoid really bad grammatical errors (feature mismatches). Next, we prepare and run the simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Condition: canoe-kayaks\n",
      "Of 1000: [100] [200] [300] [400] [500] [600] [700] [800] [900] [1000] \n",
      "Condition: canoe-kayak\n",
      "Of 1000: [100] [200] [300] [400] [500] [600] [700] [800] [900] [1000] \n",
      "Condition: canoe-cabins\n",
      "Of 1000: [100] [200] [300] [400] [500] [600] [700] [800] [900] [1000] \n",
      "Condition: canoe-cabin\n",
      "Of 1000: [100] [200] [300] [400] [500] [600] [700] [800] [900] [1000] \n",
      "Parses:\n",
      "\n",
      "canoe-kayaks:\n",
      "\tVsg:\t0.953\n",
      "\tVpl:\t0.047\n",
      "canoe-kayak:\n",
      "\tVsg:\t1.0\n",
      "canoe-cabins:\n",
      "\tVsg:\t0.995\n",
      "\tVpl:\t0.005\n",
      "canoe-cabin:\n",
      "\tVsg:\t1.0\n",
      "\n",
      "Mean =\t[ 161.485  185.079  104.474  106.183]\n",
      "SD =\t[ 195.018  129.018   73.008   87.063]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cond_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dfcb22931448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Density plot of settling times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkdeplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Density plot of times to settle to an attractor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cond_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Defining phi function\n",
    "def phi(x, center, gamma):\n",
    "    diff = x - center\n",
    "    l2norm = np.sqrt(np.dot(diff, diff))\n",
    "    phi = np.exp(-l2norm**2 / gamma)\n",
    "    return phi\n",
    "\n",
    "\n",
    "# A function for updating the state of the system according to the negative\n",
    "# gradient of the harmony function\n",
    "def step_dyn(x, centers, harmonies, gamma):\n",
    "    dx = np.zeros(x.shape)\n",
    "    for c in range(centers.shape[0]):\n",
    "        dx += (-2./gamma * harmonies[c]\n",
    "               * (x - centers[c,:]) * phi(x, centers[c,:], gamma))\n",
    "    return dx\n",
    "\n",
    "\n",
    "# Proximity to a fixed point\n",
    "def not_close(x, centers, tol):\n",
    "    for c in range(centers.shape[0]):\n",
    "        diff = x - centers[c]\n",
    "        l2norm = np.sqrt(np.dot(diff, diff))\n",
    "        if l2norm < tol:\n",
    "            #print(centers[c])\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "# Find out which fp. the system reached\n",
    "def which_attr(x):\n",
    "    x = np.round(x)   # Note:  assumes all centers are at corners of [0, 1]^ndim\n",
    "    for c in range(centers.shape[0]):\n",
    "        if np.all(x == centers[c,]):\n",
    "            return c\n",
    "    return -1\n",
    "\n",
    "\n",
    "# Singular or plural verb?\n",
    "def sg_pl(x):\n",
    "    x = np.round(x)\n",
    "    if x[-2] == 0:\n",
    "        return 0\n",
    "    elif x[-2] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Setting parameters\n",
    "gamma = 0.5\n",
    "maxsteps = 1000\n",
    "tau = 0.01\n",
    "tol = 0.2\n",
    "noisemag = 0.05\n",
    "nruns = 1000\n",
    "\n",
    "# Running\n",
    "rts = np.zeros((nruns, ncond))\n",
    "parses = np.zeros((nruns, ncond))\n",
    "for cond in range(ncond):\n",
    "    print('\\nCondition: {}'.format(condition_labels[cond]))\n",
    "    print('Of {}: '.format(nruns), end='')\n",
    "    for run in range(nruns):\n",
    "        if (run+1) % 100 == 0:\n",
    "            print('[{}] '.format(run+1), end='')\n",
    "        x = np.zeros((maxsteps, centers.shape[1]))\n",
    "        x[0,] = x0\n",
    "        if nruns is not 1:\n",
    "            noise = np.random.normal(0, 1, x.shape)\n",
    "        else:\n",
    "            noise = np.zeros(x.shape)\n",
    "\n",
    "        t = 0\n",
    "        while t < maxsteps-1:\n",
    "            if not_close(x[t,], centers, tol = tol):\n",
    "                x[t+1,] = x[t,]+(tau*(step_dyn(x[t,], centers, harmony_vals[:,cond], gamma))\n",
    "                                 + np.sqrt(noisemag*tau)*noise[t,])\n",
    "                t += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        xtrunc = x[~np.all(x == 0, axis=1)]\n",
    "        rts[run, cond] = xtrunc.shape[0]  # Time is noun attraction time, not verb time (???)\n",
    "        #parses[run, cond] = which_attr(xtrunc[-1,])\n",
    "        parses[run, cond] = sg_pl(xtrunc[-1,])\n",
    "    if nruns == 1:\n",
    "        plt.plot(xtrunc[:,ndim-1], xtrunc[:,0], label=condition_labels[cond])  # Boat-Attach\n",
    "        ftitle = 'Boat vs. Attachment'# Boat-Number\n",
    "    \n",
    "if nruns == 1:\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.legend()\n",
    "    plt.title(ftitle)\n",
    "    plt.show()\n",
    "    print(rts[0,])\n",
    "else:\n",
    "    print('\\nParses:\\n')\n",
    "    parse_labels = ['Vsg', 'Vpl', 'other']\n",
    "    for cond in range(ncond):\n",
    "        uniq, cts = np.unique(parses[:,cond], return_counts=True)\n",
    "        print('{}:'.format(condition_labels[cond]))\n",
    "        for u in range(len(uniq)):\n",
    "            if cts[u] is not 0:\n",
    "                print('\\t{}:\\t{}'.format(parse_labels[u], cts[u]/nruns))\n",
    "    print('\\nMean =\\t{}\\nSD =\\t{}'.format(np.mean(rts, axis=0), np.round(np.std(rts, axis=0), 3)))\n",
    "    for cond in range(4):\n",
    "        # Density plot of settling times\n",
    "        sns.kdeplot(rts[:,cond], label=condition_labels[cond])\n",
    "    plt.title('Density plot of times to settle to an attractor')\n",
    "    plt.ylim(0, 0.025)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Overall, we this simulation replicates Barker et al. (2001)'s finding of increased agreement attraction when N1 and N2 are semantically similar compared to when they are semantically dissimilar. Moreover, it makes the novel prediction that reading times at the verb should be slower in the similar condition than in the dissimilar condition. This prediction has received initial support from new self-paced reading data in Smith, Franck, and Tabor (in progress), where we found a main effect of semantic similarity at the verb in the same direction as the simulations here.\n",
    "\n",
    "What’s happening in this simulation is that the model is mostly building a parse where it attaches the verb to N1, but sometimes attaching to N2 and it's generating a verb that matches the features of N2 on the Boat and Number dimensions.  It does this relatively often for *canoe by the kayaks* because of it's initial journey in the direction of forming the parse for *canoe is* starts turning on all the +Boat features and thus brings it somewhat near the state that it need to be in to take *kayaks* as a subject (the noise therefore sometimes manages to bump into this state.). It does this relatively rarely for *canoe by the cabins* because *cabins* needs all the boat features turned off so the state of having *cabins* as subject is further away (it is not as likely that the noise to jostle it into this state).\n",
    "\n",
    "Initially, the state starts off completely unbiased in all features, then canoe is read, and this pushes the features a little bit toward +Boat, -Plural. Then N2 is read, and the potential of attaching the verb to N2 pulls the features in various directions. When N2 is *kayak*, the system is pulled toward attachment to N2 with treelet features at [+Boat (x3), -Plural]. This state is quite near attachment to N1 (the correct attachment)---the only thing that’s different about it is the link value and the missing attachments to N1 so it exerts very strong competition. When N2 is *kayaks*, the pull is still strong, but a little less so because the plural feature of *kayaks* makes N2 attachment less similar to N1 attachment. But if N2 is either *cabin* or *cabins*, the pull is relatively weak since all the semantic feature differences make these parses quite different from the current state. After some settling under the influence of N2, the verb itself is read. In order to finish processing the verb, the system must get reasonably close (tol = 0.2, Euclidean distance) to a stable state. The verb, which is *is*, exerts a force that pushes the system toward the singular state. In the condition *canoe near kayak*, there is high resistance to the force of *is* from the possibility of making *kayak* the subject. In the condition *canoe near kayaks*, there is still quite a bit of resistance, though it's a little less, as described above. In the *cabin* and *cabins* condition, there is relatively little resistance.   In other words, *kayak* is slowest, *kayaks* is faster, but still pretty slow; *cabin* and *cabins* are both faster.    (Note that the time to process *is* is closely correlated with the time it would have taken the system to reach the parse of *is* based on just perceiving *(the) N1 (near the) N2*. Because it was simpler to program, the times reported above are actually simply the times to convergence after processing N2).\n",
    "\n",
    "It is true that the model actually generates both a main effect of Similarity and possibly an interaction between N2 number and semantic similarity, whereas we only observed a main effect of similarity. We assume this is because the experimental design was not powerful enough to detect the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

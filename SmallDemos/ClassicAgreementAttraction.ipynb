{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of a simple SOSP model of classic agreement attraction\n",
    "\n",
    "## Introduction\n",
    "Starting with a featural lexical-dependency grammar, we design a Harmony Function (HF).  The function is defined on a space of link and treelet feature variables.  The gradient of HF gives the dynamics of processing, i.e., it specifies the way the link and feature values change from any initial state.  Sentence comprehension consists of (c1) starting with the system in an initial state in which all variables have the value 0, (c2) perceiving a word, which results in the setting of a subset of treelet features to non-zero values, (c3) gravitating under the dynamics + noise to a low velocity threshold (this generally results in nearly reaching a local harmony peak), and repeating from step (c2) until all the words in the sequence to be comprehended have been processed.  Random production (useful for probing the non-contextualized distribution over parse trajectories) is accomplished by (p1) starting in the zero state, (p2) using the prior of initial word likelihoods to select a word, (p3) activating the treelet features associated with that word, (p4) gravitating under the dynamics + noise to the low velocity threshold, (p5) generating a distribution over next words from the ensemble of open lexical attachment site activations, (p6) generating a new word, and repeating from step (p3) until (p4) has produced maximal harmony or a practical limit on sentence length has been exceeded.  (The case of production of language in a discourse context will be discussed separately.)\n",
    "\n",
    "In this case, we focus on the process of producing a number-inflected verb after already having produced a subject NP of the form *The N1 Prep the N2*. This models the classic sentence completion paradigm of Bock & Miller (1991) and many subsequent experiments. The general finding is that people produce a more incorrect plural verbs following *the N1[sg] Prep the N2[pl]* than *the N1[sg] Prep the N2[sg]*.\n",
    "\n",
    "## Grammar, representation, & dynamics\n",
    "In a fuller model, the grammar is a collection of lexically anchored treelets. **The relation is not one of \"less full\" vs. \"more full\": rather, the treelet view is a different interpretation of the same model** ** *I don't fully understand this comment. What we're basically simulating here is a subset of the dimensions in the full treelet model, specifically just the number features on each treelet with some assumptions about which parse is forming in each case.* ** Each treelet consists of a mother and a finite number of daughters, some of which may be marked \"optional\". The mother and daughters are vectors of features, all of the same dimension, with the same interpretation assigned to each dimension. The harmony of a parse (which corresponds to an attractor at a harmony peak) is determined by the feature match between the attachment sites on linked treelets. Perfect feature match results in a harmony value of 1.0, with lower harmony values for feature mismatches and failures to attach required dependents.\n",
    "\n",
    "To illustrate the dynamics of choosing a verb after having processed the subject NP, simplify the state space down to just three dimenions. The three dimensions represent the number markings on N1, N2, and the verb, with 0 coding singular (S) and 1 coding plural (P). Thus, each corner of the unit hypercube is a simple representation of a possible parse (including both grammatical and ungrammatical sequences of words). We define these points (which will be fixed points of the processing dynamics below). The following figure illustrates the state space: **I've put the figure in the figure directory but this command doesn't seem to be suitable for showing it...** ** *It seems to be working for me. Is there any error message for you, or just a blank space?* **\n",
    "\n",
    "![title](Figures/AgrAttrStateSpace.pdf)\n",
    "\n",
    "The size of the dot at a vertex represents the relative magnitude of its harmony value, as discussed next. The X marks the position of the initial state of the system for the simulations below, $\\mathbf{s}_0$. The different conditions are modeled by choosing different initial conditions $\\mathbf{s}_0$. $\\mathbf{s}_0$ encodes what linguistic information has come before. $[0, 1, 0.5]$, e.g., is the starting point for choosing a verb after producing a singular N1 and a plural N2, and $[1, 0, 0.5]$ is the starting point for N1[pl]-N2[sg]. By starting the verb's number feature halfway between singular and plural, we model verb number selection by allowing the system to settle to one attractor or another given the previous input, the harmony landscape, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up an array of fixed points (centers of the summed Gaussians discussed below)\n",
    "import numpy as np\n",
    "\n",
    "centers = np.array([[0, 0, 0], # SSS\n",
    "                   [0, 0, 1],  # SSP\n",
    "                   [0, 1, 0],  # SPS\n",
    "                   [0, 1, 1],  # SPP\n",
    "                   [1, 0, 0],  # PSS\n",
    "                   [1, 0, 1],  # PSP\n",
    "                   [1, 1, 0],  # PPS\n",
    "                   [1, 1, 1]])  # PPP\n",
    "center_labels = ['SSS', 'SSP', 'SPS', 'SPP', 'PSS', 'PSP', 'PPS', 'PPP', 'other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The harmony values for each of these vertices was determined in the following way. We assume that correct parses, i.e., treelets attaching in ways best licensed by the grammar with good feature matches, have the maximal harmony value $h_i$ of 1.0. For less-than-correct parses, we assume that partial parses (with otherwise good feature matches) are have much lower $h_i$ than correct parses, and that parses with a feature clash have the lowest $h_i$, i.e., $h_{i, correct} \\gg h_{i, partial} \\gg h_{i, clash}$. This amounts to saying that if the parser can't build a perfect parse, it strongly prefers a partial parse (a treelet *not* attaching anywhere) to a parse in which there is a feature clash. We chose to penalize partial parses by setting the relevant $h_i$s to 0.2 and feature clashes to 0.01, but the system seems to behave reasonably as long as the ordering is preserved.\n",
    "\n",
    "We assume for the case at hand that the harmony values associated with each parse are the highest ones possible for that configuration:\n",
    "- Correct parses (SSS, SPS, PSP, and PPP) have a harmony value of 1.0.\n",
    "- For SPP and PSS, the parser prefers to *not* attach N1 (instead attaching a number-matching N2), resulting in an $h_i$ of 0.2.\n",
    "- Finally, for SSP and PPS, the best option is to attach the N1 as subject despite its feature clash with the verb, making $h_i = 0.1$.\n",
    "\n",
    "There are other ways of building each parse; e.g., PPS could result from N1 failing to attach, but that parse would be penalized twice, once for failure to attach and once for the feature clash between N2 (attached as the subject) and the verb. This would result in $h_i = 0.2 * 0.01 = 0.02$ (we assume the penalties for multiple violations are multiplied). Since there is a higher-harmony parse available (attaching N1 with feature clash), we assume that the parser chooses that higher-harmony parse instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The order of the harmony values corresponds to the order of the centers in the last code block.\n",
    "# ** To make this a document to share, just report one choice or characterize a range of behaviors.  \n",
    "# ** Reporting precisely these two cases seems arbitrary\n",
    "\n",
    "# produced LARGE amounts of attraction in the PS and PP conditions:\n",
    "#harmony_values = np.array([1, 0.01, 1, 0.2, 0.2, 2, 0.01, 2])\n",
    "# produced reasonable rates of attraction, but no sg./pl. asymmetry\n",
    "harmony_values = np.array([1, 0.01, 1, 0.2, 0.2, 1, 0.01, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the global harmony function, HF, out of the local harmony values. The global harmony function, $HF(\\mathbf{s})$, is constructed by placing a Gaussian RBF, $\\phi_i(\\mathbf{s})$ at each local harmony locus $i$, scaled by the local harmony of that  $h_i$ (Muezzinoglu & Zurada, 2006):\n",
    "\n",
    "$$HF(\\mathbf{s}) = \\sum_{i \\in Centers} h_i\\ \\phi_i(\\mathbf{s})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\phi_i(\\mathbf{s}) = \\exp{\\left(-\\frac{\\Vert\\mathbf{s} - \\mathbf{c}_i\\Vert^2_2}{\\gamma}\\right)}$$\n",
    "\n",
    "Here, $\\mathbf{s}$ is the system state, i.e., the vector of activations of all the links and features. Each of these values ranges, in principle, over $(-\\infty, \\infty)$, but because of the design of the harmony function, the values will stay close the interval $[0, 1]$. $\\mathbf{c}_i$ is the center of the $i$th Gaussian, each of which defines an attractor of the system dynamics below. $\\gamma$ is a free parameter which specifies the widths of the RBFs, and thus determines how strongly the system prefers to achieve optimal parses (greater $\\gamma$ means stronger preference for maximal harmony). $\\Vert\\cdot\\Vert^2_2$ is the squared Euclidean distance (or the squared L2-norm).\n",
    "\n",
    "The dynamics of the system are given by the gradient of the harmony + noise:\n",
    "\n",
    "$$\\dot{\\mathbf{s}} = \\nabla HF + \\alpha\\cdot\\eta = -\\frac{2}{\\gamma}\\sum_{i} h_i\\ (\\mathbf{s} - \\mathbf{c}_i) \\cdot \\phi_i(\\mathbf{s})\\ + \\ \\alpha\\cdot\\eta$$\n",
    "\n",
    "where $\\dot{\\mathbf{s}}$ denotes the change in the state variable with respect to time and $\\eta$ is normally distributed noise. The magnitude, $\\alpha$, of the noise is a free parameter.\n",
    "\n",
    "By making the the system dynamics equal to the gradient of the harmony function, the system takes the steepest path possible to increase harmony (modulo the effect of the noise).\n",
    "\n",
    "In the simplified system we are currently considering, $\\mathbf{s}$ is a vector in $\\mathbb{R}^3$, there are eight centers (given in `centers` above), and the harmony values of each center are given in `harmony_values`.\n",
    "\n",
    "## Running the agreement attraction simulation\n",
    "We can now run the simulation to test whether the simplified system. Here, the different conditions are modeled by setting the system at different initial states, as discussed above. We run the system until $\\mathbf{s}$ is sufficiently close to an attractor or a maximum amount of time has passed. (Note: it takes about 10 minutes for the following code to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Condition: N1sg N2pl\n",
      "Of 500: [100] [200] [300] [400] [500] \n",
      "Condition: N1sg N2sg\n",
      "Of 500: [100] [200] [300] [400] [500] \n",
      "Condition: N1pl N2sg\n",
      "Of 500: [100] [200] [300] [400] [500] \n",
      "Condition: N1pl N2pl\n",
      "Of 500: [100] [200] [300] [400] [500] "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "tau = 0.1  # step size for the discretized (Euler forward) dynamics; this seems large...\n",
    "maxsteps = 1000  # maximum number of timesteps before terminating integration\n",
    "nruns = 500  # number of runs to do per condition\n",
    "tol = 0.1  # how close the system has to be to an attractor to stop processing\n",
    "alpha = 0.1  # noise magnitude\n",
    "gamma = 0.1  # width of RBFs\n",
    "ndim = centers.shape[1]  # number of dimensions in the state space\n",
    "\n",
    "# Conditions, specified as s0 coordinates\n",
    "conditions = np.array([[0, 1, 0.5],  # SP\n",
    "                      [0, 0, 0.5],  # SS\n",
    "                      [1, 0, 0.5],  # PS\n",
    "                      [1, 1, 0.5]])  # PP\n",
    "condition_labels = ['N1sg N2pl', 'N1sg N2sg', 'N1pl N2sg', 'N1pl N2pl']\n",
    "ncond = len(condition_labels)\n",
    "\n",
    "# Defining phi function\n",
    "def phi(x, center, gamma):\n",
    "    l2norm = np.linalg.norm(x - center)\n",
    "    phi = np.exp(-l2norm**2 / gamma)\n",
    "    return phi\n",
    "\n",
    "\n",
    "# A function for updating the state of the system according to the negative\n",
    "# gradient of the harmony function\n",
    "def step_dyn(x, centers, harmonies, gamma):\n",
    "    dx = np.zeros(x.shape)\n",
    "    for c in range(centers.shape[0]):\n",
    "        dx += harmonies[c] * (x - centers[c,]) * phi(x, centers[c,], gamma)\n",
    "    return (-2./gamma)*dx\n",
    "\n",
    "\n",
    "# Proximity to a fixed point\n",
    "def not_close(x, centers, tol):\n",
    "    for c in range(centers.shape[0]):\n",
    "        l2norm = np.linalg.norm(x - centers[c,])\n",
    "        if l2norm < tol:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "# Singular or plural verb?\n",
    "def sg_pl(x):\n",
    "    x = np.round(x)\n",
    "    if x[-1] == 0:\n",
    "        return 0\n",
    "    elif x[-1] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Find out which fp. the system reached; not currently used!\n",
    "def which_attr(x):\n",
    "    x = np.round(x)\n",
    "    for c in range(centers.shape[0]):\n",
    "        if np.all(x == centers[c,]):\n",
    "            return c\n",
    "    return -1\n",
    "\n",
    "# Doing the simulations\n",
    "parses = np.zeros((nruns, ncond))\n",
    "vnum = np.zeros((nruns, ncond))\n",
    "for cond in range(ncond):\n",
    "    print('\\nCondition: {}'.format(condition_labels[cond]))\n",
    "    print('Of {}: '.format(nruns), end='')\n",
    "    for run in range(nruns):\n",
    "        if (run+1) % 100 == 0:\n",
    "            print('[{}] '.format(run+1), end='')\n",
    "        x = np.zeros((maxsteps, ndim))\n",
    "        x[0,] = conditions[cond,]\n",
    "        noise = np.random.normal(0, 1, x.shape)\n",
    "\n",
    "        t = 0\n",
    "        while t < maxsteps-1:\n",
    "            if not_close(x[t,], centers, tol = tol):\n",
    "                x[t+1,] = x[t,] + tau*step_dyn(x[t,], centers, harmony_values, gamma) + alpha*noise[t,]\n",
    "                t += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        xtrunc = x[~np.all(x == 0, axis=1)]\n",
    "        parses[run, cond] = which_attr(xtrunc[-1,])\n",
    "        vnum[run, cond] = sg_pl(xtrunc[-1,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N1sg N2pl:\n",
      "\tParse:\tProportion of runs:\n",
      "\t---------------------------\n",
      "\tother:\t0.008\n",
      "\tSSS:\t0.054\n",
      "\tSPS:\t0.88\n",
      "\tPSP:\t0.01\n",
      "\tPPP:\t0.048\n",
      "\n",
      "\tother:\t0.006\n",
      "\tVsg:\t0.934\n",
      "\tVpl:\t0.06\n",
      "\n",
      "N1sg N2sg:\n",
      "\tParse:\tProportion of runs:\n",
      "\t---------------------------\n",
      "\tother:\t0.006\n",
      "\tSSS:\t0.968\n",
      "\tSPS:\t0.004\n",
      "\tPSP:\t0.016\n",
      "\tPPP:\t0.006\n",
      "\n",
      "\tother:\t0.006\n",
      "\tVsg:\t0.972\n",
      "\tVpl:\t0.022\n",
      "\n",
      "N1pl N2sg:\n",
      "\tParse:\tProportion of runs:\n",
      "\t---------------------------\n",
      "\tother:\t0.026\n",
      "\tSSS:\t0.042\n",
      "\tSPS:\t0.022\n",
      "\tPSP:\t0.878\n",
      "\tPPP:\t0.032\n",
      "\n",
      "\tother:\t0.016\n",
      "\tVsg:\t0.068\n",
      "\tVpl:\t0.916\n",
      "\n",
      "N1pl N2pl:\n",
      "\tParse:\tProportion of runs:\n",
      "\t---------------------------\n",
      "\tother:\t0.022\n",
      "\tSSS:\t0.018\n",
      "\tSPS:\t0.024\n",
      "\tPSP:\t0.026\n",
      "\tPPP:\t0.91\n",
      "\n",
      "\tother:\t0.02\n",
      "\tVsg:\t0.042\n",
      "\tVpl:\t0.938\n"
     ]
    }
   ],
   "source": [
    "# Now, just looking at the final distribution of parses:\n",
    "parse_labels = ['Vsg', 'Vpl', 'other']\n",
    "for cond in range(ncond):\n",
    "    uniq1, cts1 = np.unique(parses[:,cond], return_counts=True)\n",
    "    uniq2, cts2 = np.unique(vnum[:,cond], return_counts=True)\n",
    "    print('\\n{}:'.format(condition_labels[cond]))\n",
    "    print('\\tParse:\\tProportion of runs:')\n",
    "    print('\\t{}'.format(''.join(['-']*27)))\n",
    "    for u in range(len(uniq1)):\n",
    "        if cts1[u] is not 0:\n",
    "            print('\\t{}:\\t{}'.format(center_labels[int(uniq1[u])], cts1[u]/nruns))\n",
    "    print('')\n",
    "    for u in range(len(uniq2)):\n",
    "        if cts2[u] is not 0:\n",
    "            print('\\t{}:\\t{}'.format(parse_labels[int(uniq2[u])], cts2[u]/nruns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above tables show the proportion of runs that generated each type of parse in each condition. The first set of rows for each condition lists the actual parse formed, and the second set summarizes the results based on verb number only. There is a clear agreement attraction effect in the N1[sg] conditions, with N1[sg]-N2[pl] producing about 5-6% plural-verb parses and N1[sg]-N2[sg] producing about 2-3%, replicating the classic result.\n",
    "\n",
    "However, a similar attraction effect is present in the N1[pl] conditions as well, with more singular agreement for N1[pl]-N2[sg] (~5-6%) than for N1[pl]-N2[pl] (~2-4%). Thus, this model fails to replicate the singular-plural asymmetry observed in English.\n",
    "\n",
    "**Dependence on parameters and asymmetry:** I haven't explored this yet, other than the extra high harmony peaks for plural verb parses, which produced the sg./pl. asymmetry, but in the wrong direction. There was still agreement attraction in the N1[sg] cases, but for N1[pl], there were too many singular-verb parses for both N2[sg] (~26%) and N2[pl] (~13%). This is where I was unable to replicate the results of your Matlab script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

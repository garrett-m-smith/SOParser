Parser dev notes:

-Link strength has to be positive, other wise ungrammatical inputs settle on
completely wrong parses (e.g., 'many dog are' -> 'many cats are')
-Recurrent weight matrix can't have correlations between lexical nodes and
morphological nodes. This might be problematic later on... Since this is the
case, when phon. form comes, have to spec. morph. This might be a case of
spurious attractors. Trying noising the weights: didn't work.

Possible problem: It's not adding new lexical items that is making
parse formation less stable, it's changing the weight matrices to make the 
different feature banks disconnected. I think this means that I'll need a
separate lexical representation (lexical unit) for each morphological form:
'is', 'are', 'sing', 'sings', etc. Is this just for verbs? Let's start with:
    1. Det: add one new Det (with +link strengths and no lex-morph corr. in
    W_rec) works. add second new:
    2. Noun
    3. Verb

Status as of 18:24 on 10.03.:
	Still have the issue of weird determiner stable state of [-a, +these, -this, +many]. This seems to be present all the time, but I want to test all combinations of the following:
		-0 or 1 on W_rec diag
		-Noising the weights to avoid spurious attractors
		-Making the links strictly positive (via sig() fn.)
		-+/- Correlation between lexical nodes and morph nodes
	Also, check for stupid commenting in/out mistakes, which could also be possible...
	

Notes on 13.03:
	With noun_patterns just encoding the lexical features (no number), get oscillations when link dyn are absent and recurrent weights are set using pseudoinverse + w/feature bank inhib.
	-RESOLVED
	Seems to be working now....
	Next steps:
	-Test adding new verbs
	-Implement method that simplifies adding new lexical entries to the lexicon
	-Link dynamics
	-PP modifier
	
Notes, 16.03:
	Having switched to LV dynamics, the question is how to handle feature passing. My initial thought was to just add the appropriate terms to the recurrent weight matrix. However, that matrix defines the interactions in a winner-takes-all system, so having another term competing just adds to the competition, but doesn't change the winner-takes-all dynamics. Therefore, I need another way of handling feature passing. One way of doing that is to do some kind of weighted average of the head's current number feature with the input from the dependent, putting this into the input_to_X vector.
	
Notes on 24.03.:
	Tried Whit's input idea: dx/dt = x(1 - Wx)net, net = l * (x_sending - x_receiving). This makes the basis vectors unstable if net < 1. Plus, there are oscillations between 0 and 1, so that's out. Next steps: add node for "no dep", dep. bank should have all words, & LV modifications: interconnections with coefs < 0, set param k = 0.
	Got the new feature bank organization going. Now trying to decide on feature passing, etc. I think it's reasonable for the dependent bank _not_ to try to push back on the head bank of the sending treelet. Just share the number bidirectionally. The dependent bank on the head gets input from dependent head bank, but dependent head bank gets no input from head treelet dependent bank.
	
25.03:
	Added in averaging of inputs from Det and V to Agr bank of noun. Seems to behave reasonably.
	Added in lexicon of all words (just a list) and a set of for-loops to handle the updating. All of the links are now entertained, but they don't compete yet. That's the next step!

26.03:
	Link strengths are now saved at each time step, and I've added a function for plotting them. Next step is implementing the LV competition in the link dynamics. 
	Ok, links compete and feature passing works. I just need a way of making the links to dep_null go to 0...
	Well, using the LV form x(input - Wx) works to keep the links to the Dep dependent attachment site near zero, but this might be a questionable choice. Currently, the model exhibits reasonable behavior for both grammatical and ungrammatical sentences. Next step PP? Still want a way of making dep_null stable, though...
	
27.03.:
	Still the question of interconnections, otherwise, priming doesn't really work well...
	Also: intro'ing phonological form should leave features unaffected by the phon. form in their current state instead of setting them to zero.
	States now start off of the fixed points, i.e., 0 < x0 < 1.
	Ugh, see top of file...
	
13.04.:
	Not implemented idea for how to "close" attachment sites: Use a threshold a la a neural net, e.g.,
	dx/dt = x((input - threshold) - W @ (input * x))
	Thus, when there is little input to an attachment site, the link's effect growth rate will be < 0, making 0 on that dimension a stable fixed point. The threshold could be static (e.g., 0.1) or dynamic, e.g., making it equal to some proportion of the current avg. link strength or proportional to the total link activation, etc. Would need to test the dynamics rigorously, but could be done. See how original Soparse (2004) did it, as well as possibly Wilson et al. (2000).
	
31.05.:
	Have a look at Sprott et al. 2005 and Wildenberg et al. 2006 for possible way of finding Lyapunov functions for the CUNY model! See also Hirsch, Smale, & Devaney text book for 'separation of variables' approach to Lyapunov fn.
	
02.06.:
	Picking up again where I left off. First change: added a fn. to Treelets object to calculate the feature match between all linked attachment sites. Sketched dynamics functions (not running). I think I might have to switch to a different way of tracking the link strengths and treelet activations. Need one big vector or multi-dimensional array?... At the very least, I need a way of getting all of the numerical state variables into one place quickly, even if they're stored in the current dict system.

05.06.: Ok, Tried redoing the infrastructure. Tried record arrays first (might work, but I couldn't figure it out), then just simple NumPy arrays in 4D. Seems to be feasible, and relatively simple. However, in testing, w/ 2 treelets, I noticed that it's possible with "Dogs sleep" for (e.g.) "dogs" and "sleep" to both dominate each other, i.e., the link from the Det node on "dogs" can reach full activation, and so can the link from the Subj node on "sleeps." We want these sentences to form correctly, but the only thing I can come up with right now is to threshold the feature match between treelets. If the feature match is below the threshold, the link strength goes to 0. It remains to be seen if and how this interacts with treelet activations and their thresholds.
	Ok, Numpy arrays will be difficult as well. Reason: treelets can take different numbers of dependents. Either I caluculate beforehand what how many links I'll need and label them consistently, or I switch back to structured/record arrays, where I can create rows with just the info I need. Just gotta figure out the fucking indexing...
